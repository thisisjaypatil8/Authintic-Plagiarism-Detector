{
    "sourceFile": "nlp-service/preprocess_sources.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 3,
            "patches": [
                {
                    "date": 1762431644388,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1762441045244,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -5,8 +5,9 @@\n import numpy as np\r\n import pickle\r\n from tqdm import tqdm\r\n from sentence_transformers import SentenceTransformer\r\n+import nltk\r\n \r\n # Attempt to use FAISS; fallback if unavailable\r\n try:\r\n     import faiss\r\n@@ -16,86 +17,108 @@\n     class _LocalIndex:\r\n         def __init__(self, dim):\r\n             self._dim = dim\r\n             self.vectors = np.empty((0, dim), dtype='float32')\r\n-\r\n         def add(self, x):\r\n             x = np.asarray(x, dtype='float32')\r\n             if x.ndim == 1:\r\n                 x = x.reshape(1, -1)\r\n             self.vectors = np.vstack((self.vectors, x))\r\n-\r\n         @property\r\n         def ntotal(self):\r\n             return int(self.vectors.shape[0])\r\n-\r\n     class _FaissFallbackModule:\r\n         IndexFlatL2 = _LocalIndex\r\n         @staticmethod\r\n         def write_index(index, filepath):\r\n+            # Save vectors for our fallback\r\n             np.save(filepath + '.npy', index.vectors)\r\n-\r\n+            # Save a pickle of the index object for compatibility\r\n+            with open(filepath, 'wb') as f:\r\n+                pickle.dump(index, f)\r\n     faiss = _FaissFallbackModule()\r\n+    # Check if a real faiss is available\r\n+    if hasattr(faiss, 'Index'):\r\n+        print(\"‚úÖ FAISS-CPU found.\")\r\n+        faiss.write_index = faiss.write_index # Use the real one\r\n+    else:\r\n+        print(\"‚ö†Ô∏è Using NumPy fallback for FAISS.\")\r\n \r\n+\r\n # --- CONFIG ---\r\n SOURCE_FOLDER = 'source_texts'\r\n DATA_FILE = 'source_data.pkl'\r\n FAISS_INDEX_FILE = 'source_index.faiss'\r\n-BATCH_SIZE = 256\r\n+BATCH_SIZE = 512 # Can be larger for sentences\r\n EMBEDDING_DIM = 384  # For all-MiniLM-L6-v2\r\n \r\n print(\"üîπ Loading SentenceTransformer model (all-MiniLM-L6-v2)...\")\r\n model = SentenceTransformer('all-MiniLM-L6-v2')\r\n \r\n+print(\"üîπ Loading NLTK sentence tokenizer...\")\r\n+nltk.download('punkt', quiet=True)\r\n \r\n-def get_file_paths(path):\r\n+\r\n+def get_source_sentences(path):\r\n+    \"\"\"\r\n+    Reads all .txt files, splits them into sentences,\r\n+    and returns a list of (sentence, source_name) tuples.\r\n+    \"\"\"\r\n     print(f\"Scanning for source files in: {path}\")\r\n-    return glob.glob(os.path.join(path, '*.txt'))\r\n+    file_paths = glob.glob(os.path.join(path, '*.txt'))\r\n+    \r\n+    all_sentences = []\r\n+    source_names = [] # This will map 1-to-1 with all_sentences\r\n \r\n-\r\n-def process_batch(file_paths):\r\n-    \"\"\"Reads a batch of files and computes embeddings.\"\"\"\r\n-    texts, names = [], []\r\n-    for path in file_paths:\r\n+    print(f\"Tokenizing {len(file_paths)} documents...\")\r\n+    for path in tqdm(file_paths, desc=\"Reading files\"):\r\n         try:\r\n             with open(path, 'r', encoding='utf-8', errors='ignore') as f:\r\n-                texts.append(f.read())\r\n-                names.append(os.path.basename(path))\r\n+                text = f.read()\r\n+                sentences = nltk.sent_tokenize(text)\r\n+                base_name = os.path.basename(path)\r\n+                \r\n+                for s in sentences:\r\n+                    s = s.strip()\r\n+                    if len(s) > 10: # Only index sentences > 10 chars\r\n+                        all_sentences.append(s)\r\n+                        source_names.append(base_name)\r\n+                        \r\n         except Exception as e:\r\n             print(f\"‚ö†Ô∏è  Could not read {path}. Skipping. Error: {e}\")\r\n \r\n-    if not texts:\r\n-        return None, None\r\n+    return all_sentences, source_names\r\n \r\n-    embeddings = model.encode(texts, show_progress_bar=False)\r\n-    return names, embeddings\r\n \r\n-\r\n if __name__ == '__main__':\r\n-    file_paths = get_file_paths(SOURCE_FOLDER)\r\n-    if not file_paths:\r\n-        print(f\"‚ùå No .txt files found in '{SOURCE_FOLDER}'. Exiting.\")\r\n+    \r\n+    sentences, source_names = get_source_sentences(SOURCE_FOLDER)\r\n+    \r\n+    if not sentences:\r\n+        print(f\"‚ùå No valid sentences found in '{SOURCE_FOLDER}'. Exiting.\")\r\n         exit()\r\n \r\n-    print(f\"‚úÖ Found {len(file_paths)} source documents. Starting batch processing...\")\r\n+    print(f\"‚úÖ Found {len(sentences)} total sentences. Starting embedding...\")\r\n \r\n     faiss_index = faiss.IndexFlatL2(EMBEDDING_DIM)\r\n-    all_names = []\r\n+    \r\n+    # Process in batches\r\n+    for i in tqdm(range(0, len(sentences), BATCH_SIZE), desc=\"Processing Batches\"):\r\n+        batch_sentences = sentences[i:i + BATCH_SIZE]\r\n+        embeddings = model.encode(batch_sentences, show_progress_bar=False)\r\n+        faiss_index.add(np.array(embeddings).astype('float32'))\r\n \r\n-    for i in tqdm(range(0, len(file_paths), BATCH_SIZE), desc=\"Processing Batches\"):\r\n-        batch_paths = file_paths[i:i + BATCH_SIZE]\r\n-        names, embeddings = process_batch(batch_paths)\r\n-        if names:\r\n-            faiss_index.add(np.array(embeddings))\r\n-            all_names.extend(names)\r\n-\r\n     if faiss_index.ntotal == 0:\r\n-        print(\"‚ùå No valid documents processed. Exiting.\")\r\n+        print(\"‚ùå No valid sentences processed. Exiting.\")\r\n         exit()\r\n \r\n     print(\"\\nüíæ Saving processed data...\")\r\n+    # --- NEW: Save the source_names list ---\r\n+    # This list directly maps to the FAISS index\r\n+    # (e.g., index 5 in FAISS corresponds to source_names[5])\r\n     with open(DATA_FILE, 'wb') as f:\r\n-        pickle.dump(all_names, f)\r\n+        pickle.dump(source_names, f)\r\n \r\n     faiss.write_index(faiss_index, FAISS_INDEX_FILE)\r\n-    print(f\"‚úÖ Saved FAISS index and {len(all_names)} source names successfully.\")\r\n-    print(\"\\nüéØ Preprocessing complete ‚Äî ready to start `app.py`.\")\r\n+    \r\n+    print(f\"‚úÖ Saved FAISS index and {len(source_names)} source names successfully.\")\r\n+    print(\"\\nüéØ Preprocessing complete ‚Äî ready to start `app.py`.\")\n\\ No newline at end of file\n"
                },
                {
                    "date": 1762524906429,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,124 +1,72 @@\n-# nlp-service/preprocess_sources.py\r\n-\r\n import os\r\n import glob\r\n+import pickle\r\n import numpy as np\r\n-import pickle\r\n+import faiss\r\n+import nltk\r\n+from sentence_transformers import SentenceTransformer\r\n from tqdm import tqdm\r\n-from sentence_transformers import SentenceTransformer\r\n-import nltk\r\n \r\n-# Attempt to use FAISS; fallback if unavailable\r\n-try:\r\n-    import faiss\r\n-except Exception:\r\n-    print(\"‚ö†Ô∏è  'faiss' not installed ‚Äî using slower numpy fallback. Install via:\")\r\n-    print(\"   pip install faiss-cpu\")\r\n-    class _LocalIndex:\r\n-        def __init__(self, dim):\r\n-            self._dim = dim\r\n-            self.vectors = np.empty((0, dim), dtype='float32')\r\n-        def add(self, x):\r\n-            x = np.asarray(x, dtype='float32')\r\n-            if x.ndim == 1:\r\n-                x = x.reshape(1, -1)\r\n-            self.vectors = np.vstack((self.vectors, x))\r\n-        @property\r\n-        def ntotal(self):\r\n-            return int(self.vectors.shape[0])\r\n-    class _FaissFallbackModule:\r\n-        IndexFlatL2 = _LocalIndex\r\n-        @staticmethod\r\n-        def write_index(index, filepath):\r\n-            # Save vectors for our fallback\r\n-            np.save(filepath + '.npy', index.vectors)\r\n-            # Save a pickle of the index object for compatibility\r\n-            with open(filepath, 'wb') as f:\r\n-                pickle.dump(index, f)\r\n-    faiss = _FaissFallbackModule()\r\n-    # Check if a real faiss is available\r\n-    if hasattr(faiss, 'Index'):\r\n-        print(\"‚úÖ FAISS-CPU found.\")\r\n-        faiss.write_index = faiss.write_index # Use the real one\r\n-    else:\r\n-        print(\"‚ö†Ô∏è Using NumPy fallback for FAISS.\")\r\n-\r\n-\r\n-# --- CONFIG ---\r\n-SOURCE_FOLDER = 'source_texts'\r\n+# --- SETUP ---\r\n+model = SentenceTransformer('all-MiniLM-L6-v2')\r\n+SOURCE_DIR = \"source_texts\"\r\n DATA_FILE = 'source_data.pkl'\r\n FAISS_INDEX_FILE = 'source_index.faiss'\r\n-BATCH_SIZE = 512 # Can be larger for sentences\r\n-EMBEDDING_DIM = 384  # For all-MiniLM-L6-v2\r\n \r\n-print(\"üîπ Loading SentenceTransformer model (all-MiniLM-L6-v2)...\")\r\n-model = SentenceTransformer('all-MiniLM-L6-v2')\r\n+# Download NLTK 'punkt' tokenizer if not present\r\n+try:\r\n+    nltk.data.find('tokenizers/punkt')\r\n+except LookupError:\r\n+    nltk.download('punkt')\r\n \r\n-print(\"üîπ Loading NLTK sentence tokenizer...\")\r\n-nltk.download('punkt', quiet=True)\r\n+# --- DATA CONTAINERS ---\r\n+all_source_sentences = []\r\n+source_metadata = [] # Stores (filename, sentence_index) for each sentence\r\n \r\n+# --- 1. LOAD AND SPLIT ALL DOCUMENTS ---\r\n+print(f\"Loading and sentence-splitting all files from {SOURCE_DIR}...\")\r\n+source_files = sorted(glob.glob(f\"{SOURCE_DIR}/*.txt\"))\r\n \r\n-def get_source_sentences(path):\r\n-    \"\"\"\r\n-    Reads all .txt files, splits them into sentences,\r\n-    and returns a list of (sentence, source_name) tuples.\r\n-    \"\"\"\r\n-    print(f\"Scanning for source files in: {path}\")\r\n-    file_paths = glob.glob(os.path.join(path, '*.txt'))\r\n-    \r\n-    all_sentences = []\r\n-    source_names = [] # This will map 1-to-1 with all_sentences\r\n+if not source_files:\r\n+    print(f\"ERROR: No .txt files found in '{SOURCE_DIR}'. Please add source files.\")\r\n+    exit()\r\n \r\n-    print(f\"Tokenizing {len(file_paths)} documents...\")\r\n-    for path in tqdm(file_paths, desc=\"Reading files\"):\r\n-        try:\r\n-            with open(path, 'r', encoding='utf-8', errors='ignore') as f:\r\n-                text = f.read()\r\n-                sentences = nltk.sent_tokenize(text)\r\n-                base_name = os.path.basename(path)\r\n-                \r\n-                for s in sentences:\r\n-                    s = s.strip()\r\n-                    if len(s) > 10: # Only index sentences > 10 chars\r\n-                        all_sentences.append(s)\r\n-                        source_names.append(base_name)\r\n-                        \r\n-        except Exception as e:\r\n-            print(f\"‚ö†Ô∏è  Could not read {path}. Skipping. Error: {e}\")\r\n+for filepath in source_files:\r\n+    filename = os.path.basename(filepath)\r\n+    print(f\"Processing: {filename}\")\r\n+    try:\r\n+        with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\r\n+            text = f.read()\r\n+            sentences = nltk.sent_tokenize(text)\r\n \r\n-    return all_sentences, source_names\r\n+            for i, sentence in enumerate(sentences):\r\n+                if len(sentence.strip()) > 10: # Only process meaningful sentences\r\n+                    all_source_sentences.append(sentence.strip())\r\n+                    source_metadata.append( (filename, i) ) # Link sentence to file and its index\r\n+    except Exception as e:\r\n+        print(f\"Could not read {filename}: {e}\")\r\n \r\n\\ No newline at end of file\n+print(f\"\\nFound {len(all_source_sentences)} total sentences from {len(source_files)} documents.\")\r\n \r\n-if __name__ == '__main__':\r\n-    \r\n-    sentences, source_names = get_source_sentences(SOURCE_FOLDER)\r\n-    \r\n-    if not sentences:\r\n-        print(f\"‚ùå No valid sentences found in '{SOURCE_FOLDER}'. Exiting.\")\r\n-        exit()\r\n+# --- 2. COMPUTE EMBEDDINGS ---\r\n+print(\"Computing embeddings for all sentences (this may take a while)...\")\r\n+embs = model.encode(all_source_sentences, convert_to_numpy=True, show_progress_bar=True)\r\n+embs = embs.astype('float32') # FAISS requires float32\r\n \r\n-    print(f\"‚úÖ Found {len(sentences)} total sentences. Starting embedding...\")\r\n+# --- 3. NORMALIZE & BUILD FAISS INDEX (FOR COSINE SIMILARITY) ---\r\n+print(\"Normalizing embeddings and building FAISS index...\")\r\n+faiss.normalize_L2(embs) # Normalize vectors\r\n \r\n-    faiss_index = faiss.IndexFlatL2(EMBEDDING_DIM)\r\n-    \r\n-    # Process in batches\r\n-    for i in tqdm(range(0, len(sentences), BATCH_SIZE), desc=\"Processing Batches\"):\r\n-        batch_sentences = sentences[i:i + BATCH_SIZE]\r\n-        embeddings = model.encode(batch_sentences, show_progress_bar=False)\r\n-        faiss_index.add(np.array(embeddings).astype('float32'))\r\n+d = embs.shape[1]\r\n+index = faiss.IndexFlatIP(d) # Inner Product on normalized vectors = Cosine Similarity\r\n+index.add(embs)\r\n \r\n-    if faiss_index.ntotal == 0:\r\n-        print(\"‚ùå No valid sentences processed. Exiting.\")\r\n-        exit()\r\n+# --- 4. SAVE ALL ARTIFACTS ---\r\n+faiss.write_index(index, FAISS_INDEX_FILE)\r\n \r\n-    print(\"\\nüíæ Saving processed data...\")\r\n-    # --- NEW: Save the source_names list ---\r\n-    # This list directly maps to the FAISS index\r\n-    # (e.g., index 5 in FAISS corresponds to source_names[5])\r\n-    with open(DATA_FILE, 'wb') as f:\r\n-        pickle.dump(source_names, f)\r\n+# Save the sentence list and its metadata\r\n+with open(DATA_FILE, 'wb') as f:\r\n+    pickle.dump((all_source_sentences, source_metadata), f)\r\n \r\n-    faiss.write_index(faiss_index, FAISS_INDEX_FILE)\r\n-    \r\n-    print(f\"‚úÖ Saved FAISS index and {len(source_names)} source names successfully.\")\r\n-    print(\"\\nüéØ Preprocessing complete ‚Äî ready to start `app.py`.\")\n+print(\"\\nPreprocessing complete!\")\r\n+print(f\"Files created: {FAISS_INDEX_FILE}, {DATA_FILE}\")\n\\ No newline at end of file\n"
                },
                {
                    "date": 1762524913440,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,72 +1,117 @@\n+# nlp-service/preprocess_sources.py\r\n+\r\n import os\r\n import glob\r\n-import pickle\r\n import numpy as np\r\n-import faiss\r\n-import nltk\r\n from sentence_transformers import SentenceTransformer\r\n-from tqdm import tqdm\r\n+import torch\r\n+import pickle\r\n+from tqdm import tqdm # For a nice progress bar\r\n \r\n-# --- SETUP ---\r\n-model = SentenceTransformer('all-MiniLM-L6-v2')\r\n-SOURCE_DIR = \"source_texts\"\r\n-DATA_FILE = 'source_data.pkl'\r\n-FAISS_INDEX_FILE = 'source_index.faiss'\r\n-\r\n-# Download NLTK 'punkt' tokenizer if not present\r\n+# Try to import faiss; if unavailable, provide a minimal numpy-based fallback\r\n try:\r\n-    nltk.data.find('tokenizers/punkt')\r\n-except LookupError:\r\n-    nltk.download('punkt')\r\n+    import faiss\r\n+except Exception:\r\n+    print(\"Warning: 'faiss' not installed; falling back to a slower numpy-based index. \"\r\n+          \"Install 'faiss-cpu' via pip or 'conda install -c conda-forge faiss-cpu' for better performance.\")\r\n+    class _LocalIndex:\r\n+        def __init__(self, dim):\r\n+            self._dim = dim\r\n+            self.vectors = np.empty((0, dim), dtype='float32')\r\n \r\n-# --- DATA CONTAINERS ---\r\n-all_source_sentences = []\r\n-source_metadata = [] # Stores (filename, sentence_index) for each sentence\r\n+        def add(self, x):\r\n+            x = np.asarray(x, dtype='float32')\r\n+            if x.ndim == 1:\r\n+                x = x.reshape(1, -1)\r\n+            self.vectors = np.vstack((self.vectors, x))\r\n \r\n-# --- 1. LOAD AND SPLIT ALL DOCUMENTS ---\r\n-print(f\"Loading and sentence-splitting all files from {SOURCE_DIR}...\")\r\n-source_files = sorted(glob.glob(f\"{SOURCE_DIR}/*.txt\"))\r\n+        @property\r\n+        def ntotal(self):\r\n+            return int(self.vectors.shape[0])\r\n \r\n-if not source_files:\r\n-    print(f\"ERROR: No .txt files found in '{SOURCE_DIR}'. Please add source files.\")\r\n-    exit()\r\n+    class _FaissFallbackModule:\r\n+        IndexFlatL2 = _LocalIndex\r\n \r\n-for filepath in source_files:\r\n-    filename = os.path.basename(filepath)\r\n-    print(f\"Processing: {filename}\")\r\n-    try:\r\n-        with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\r\n-            text = f.read()\r\n-            sentences = nltk.sent_tokenize(text)\r\n+        @staticmethod\r\n+        def write_index(index, filepath):\r\n+            # Save as numpy file for fallback; use .npy extension\r\n+            np.save(filepath + '.npy', index.vectors)\r\n \r\n-            for i, sentence in enumerate(sentences):\r\n-                if len(sentence.strip()) > 10: # Only process meaningful sentences\r\n-                    all_source_sentences.append(sentence.strip())\r\n\\ No newline at end of file\n-                    source_metadata.append( (filename, i) ) # Link sentence to file and its index\r\n-    except Exception as e:\r\n-        print(f\"Could not read {filename}: {e}\")\r\n+    faiss = _FaissFallbackModule()\r\n+from tqdm import tqdm # For a nice progress bar\r\n \r\n-print(f\"\\nFound {len(all_source_sentences)} total sentences from {len(source_files)} documents.\")\r\n+# --- CONFIGURATION ---\r\n+BATCH_SIZE = 256 # Reduced batch size for lower memory usage. Adjust if needed.\r\n+SOURCE_FOLDER = 'source_texts'\r\n+DATA_FILE = 'source_data.pkl'\r\n+FAISS_INDEX_FILE = 'source_index.faiss'\r\n \r\n-# --- 2. COMPUTE EMBEDDINGS ---\r\n-print(\"Computing embeddings for all sentences (this may take a while)...\")\r\n-embs = model.encode(all_source_sentences, convert_to_numpy=True, show_progress_bar=True)\r\n-embs = embs.astype('float32') # FAISS requires float32\r\n+# --- SCRIPT START ---\r\n+print(\"Loading sentence transformer model...\")\r\n+model = SentenceTransformer('all-MiniLM-L6-v2')\r\n \r\n-# --- 3. NORMALIZE & BUILD FAISS INDEX (FOR COSINE SIMILARITY) ---\r\n-print(\"Normalizing embeddings and building FAISS index...\")\r\n-faiss.normalize_L2(embs) # Normalize vectors\r\n+def get_file_paths(path):\r\n+    print(f\"Scanning for source files in: {path}\")\r\n+    return glob.glob(os.path.join(path, '*.txt'))\r\n \r\n-d = embs.shape[1]\r\n-index = faiss.IndexFlatIP(d) # Inner Product on normalized vectors = Cosine Similarity\r\n-index.add(embs)\r\n+def process_batch_for_names_and_embeddings(file_paths):\r\n+    \"\"\"Reads a batch of files, extracts text, gets names, and generates embeddings.\"\"\"\r\n+    batch_texts = []\r\n+    batch_names = []\r\n+    for filepath in file_paths:\r\n+        try:\r\n+            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\r\n+                # We only need the text for encoding, not to store it all in memory.\r\n+                batch_texts.append(f.read()) \r\n+                batch_names.append(os.path.basename(filepath))\r\n+        except Exception as e:\r\n+            print(f\"Warning: Could not read file {filepath}. Skipping. Error: {e}\")\r\n+            \r\n+    if not batch_texts:\r\n+        return None, None\r\n+        \r\n+    # Generate embeddings for the current batch\r\n+    batch_embeddings = model.encode(batch_texts, show_progress_bar=False)\r\n+    return batch_names, batch_embeddings\r\n \r\n-# --- 4. SAVE ALL ARTIFACTS ---\r\n-faiss.write_index(index, FAISS_INDEX_FILE)\r\n+if __name__ == '__main__':\r\n+    # We will only store the names in memory, not the full text of every document.\r\n+    all_source_names = []\r\n+    faiss_index = None\r\n+    embedding_dim = 384 # Dimension for 'all-MiniLM-L6-v2' model\r\n \r\n-# Save the sentence list and its metadata\r\n-with open(DATA_FILE, 'wb') as f:\r\n-    pickle.dump((all_source_sentences, source_metadata), f)\r\n+    file_paths = get_file_paths(SOURCE_FOLDER)\r\n+    \r\n+    if not file_paths:\r\n+        print(f\"Error: No .txt files found in the '{SOURCE_FOLDER}' directory. Exiting.\")\r\n+        exit()\r\n \r\n-print(\"\\nPreprocessing complete!\")\r\n-print(f\"Files created: {FAISS_INDEX_FILE}, {DATA_FILE}\")\n+    print(f\"Found {len(file_paths)} source documents. Starting batch processing...\")\r\n+\r\n+    # Initialize a FAISS index\r\n+    faiss_index = faiss.IndexFlatL2(embedding_dim)\r\n+\r\n+    # Process files in batches\r\n+    for i in tqdm(range(0, len(file_paths), BATCH_SIZE), desc=\"Processing Batches\"):\r\n+        batch_paths = file_paths[i:i + BATCH_SIZE]\r\n+        names, embeddings = process_batch_for_names_and_embeddings(batch_paths)\r\n+        \r\n+        if names:\r\n+            # Add the embeddings from this batch directly to the FAISS index\r\n+            faiss_index.add(np.array(embeddings))\r\n+            # Store only the names\r\n+            all_source_names.extend(names)\r\n+\r\n+    if faiss_index.ntotal == 0:\r\n+        print(\"Error: No documents were successfully processed. Exiting.\")\r\n+        exit()\r\n+\r\n+    print(\"\\nSaving pre-processed data to files...\")\r\n+    # Save only the names list. We don't need to save the full texts.\r\n+    with open(DATA_FILE, 'wb') as f:\r\n+        pickle.dump(all_source_names, f)\r\n+    \r\n+    # Save the completed FAISS index\r\n+    faiss.write_index(faiss_index, FAISS_INDEX_FILE)\r\n+    print(f\"FAISS index and names list saved successfully with {faiss_index.ntotal} vectors.\")\r\n+    print(\"\\nPreprocessing complete. You can now start the main app.py server.\")\r\n"
                }
            ],
            "date": 1762431644388,
            "name": "Commit-0",
            "content": "# nlp-service/preprocess_sources.py\r\n\r\nimport os\r\nimport glob\r\nimport numpy as np\r\nimport pickle\r\nfrom tqdm import tqdm\r\nfrom sentence_transformers import SentenceTransformer\r\n\r\n# Attempt to use FAISS; fallback if unavailable\r\ntry:\r\n    import faiss\r\nexcept Exception:\r\n    print(\"‚ö†Ô∏è  'faiss' not installed ‚Äî using slower numpy fallback. Install via:\")\r\n    print(\"   pip install faiss-cpu\")\r\n    class _LocalIndex:\r\n        def __init__(self, dim):\r\n            self._dim = dim\r\n            self.vectors = np.empty((0, dim), dtype='float32')\r\n\r\n        def add(self, x):\r\n            x = np.asarray(x, dtype='float32')\r\n            if x.ndim == 1:\r\n                x = x.reshape(1, -1)\r\n            self.vectors = np.vstack((self.vectors, x))\r\n\r\n        @property\r\n        def ntotal(self):\r\n            return int(self.vectors.shape[0])\r\n\r\n    class _FaissFallbackModule:\r\n        IndexFlatL2 = _LocalIndex\r\n        @staticmethod\r\n        def write_index(index, filepath):\r\n            np.save(filepath + '.npy', index.vectors)\r\n\r\n    faiss = _FaissFallbackModule()\r\n\r\n# --- CONFIG ---\r\nSOURCE_FOLDER = 'source_texts'\r\nDATA_FILE = 'source_data.pkl'\r\nFAISS_INDEX_FILE = 'source_index.faiss'\r\nBATCH_SIZE = 256\r\nEMBEDDING_DIM = 384  # For all-MiniLM-L6-v2\r\n\r\nprint(\"üîπ Loading SentenceTransformer model (all-MiniLM-L6-v2)...\")\r\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\r\n\r\n\r\ndef get_file_paths(path):\r\n    print(f\"Scanning for source files in: {path}\")\r\n    return glob.glob(os.path.join(path, '*.txt'))\r\n\r\n\r\ndef process_batch(file_paths):\r\n    \"\"\"Reads a batch of files and computes embeddings.\"\"\"\r\n    texts, names = [], []\r\n    for path in file_paths:\r\n        try:\r\n            with open(path, 'r', encoding='utf-8', errors='ignore') as f:\r\n                texts.append(f.read())\r\n                names.append(os.path.basename(path))\r\n        except Exception as e:\r\n            print(f\"‚ö†Ô∏è  Could not read {path}. Skipping. Error: {e}\")\r\n\r\n    if not texts:\r\n        return None, None\r\n\r\n    embeddings = model.encode(texts, show_progress_bar=False)\r\n    return names, embeddings\r\n\r\n\r\nif __name__ == '__main__':\r\n    file_paths = get_file_paths(SOURCE_FOLDER)\r\n    if not file_paths:\r\n        print(f\"‚ùå No .txt files found in '{SOURCE_FOLDER}'. Exiting.\")\r\n        exit()\r\n\r\n    print(f\"‚úÖ Found {len(file_paths)} source documents. Starting batch processing...\")\r\n\r\n    faiss_index = faiss.IndexFlatL2(EMBEDDING_DIM)\r\n    all_names = []\r\n\r\n    for i in tqdm(range(0, len(file_paths), BATCH_SIZE), desc=\"Processing Batches\"):\r\n        batch_paths = file_paths[i:i + BATCH_SIZE]\r\n        names, embeddings = process_batch(batch_paths)\r\n        if names:\r\n            faiss_index.add(np.array(embeddings))\r\n            all_names.extend(names)\r\n\r\n    if faiss_index.ntotal == 0:\r\n        print(\"‚ùå No valid documents processed. Exiting.\")\r\n        exit()\r\n\r\n    print(\"\\nüíæ Saving processed data...\")\r\n    with open(DATA_FILE, 'wb') as f:\r\n        pickle.dump(all_names, f)\r\n\r\n    faiss.write_index(faiss_index, FAISS_INDEX_FILE)\r\n    print(f\"‚úÖ Saved FAISS index and {len(all_names)} source names successfully.\")\r\n    print(\"\\nüéØ Preprocessing complete ‚Äî ready to start `app.py`.\")\r\n"
        }
    ]
}