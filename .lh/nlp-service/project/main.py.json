{
    "sourceFile": "nlp-service/project/main.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 2,
            "patches": [
                {
                    "date": 1762593757054,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1762594151721,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,7 +1,8 @@\n import requests\r\n import numpy as np\r\n import faiss\r\n+import nltk  # <-- THIS WAS THE MISSING LINE\r\n from flask import request, jsonify, Blueprint\r\n from .loader import model, index, source_sentences, source_metadata\r\n \r\n # Create a 'Blueprint' - a collection of routes that can be registered with the app\r\n@@ -13,9 +14,9 @@\n     if not data or 'text' not in data:\r\n         return jsonify({'error': 'No text provided'}), 400\r\n \r\n     user_text = data['text']\r\n-    user_sentences = nltk.sent_tokenize(user_text) # We need to import nltk in this file\r\n+    user_sentences = nltk.sent_tokenize(user_text) # This will now work\r\n \r\n     if not user_sentences or not index:\r\n         return jsonify({'overall_score': 0, 'flagged_sections': [], 'full_text_structured': []})\r\n \r\n"
                },
                {
                    "date": 1762596206874,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,12 +1,12 @@\n import requests\r\n import numpy as np\r\n import faiss\r\n-import nltk  # <-- THIS WAS THE MISSING LINE\r\n+import nltk\r\n from flask import request, jsonify, Blueprint\r\n from .loader import model, index, source_sentences, source_metadata\r\n \r\n-# Create a 'Blueprint' - a collection of routes that can be registered with the app\r\n+# Create a 'Blueprint'\r\n bp = Blueprint('main', __name__)\r\n \r\n @bp.route('/api/check', methods=['POST'])\r\n def analyze_document():\r\n@@ -14,80 +14,118 @@\n     if not data or 'text' not in data:\r\n         return jsonify({'error': 'No text provided'}), 400\r\n \r\n     user_text = data['text']\r\n-    user_sentences = nltk.sent_tokenize(user_text) # This will now work\r\n+    user_sentences = nltk.sent_tokenize(user_text)\r\n+    total_sentences = len(user_sentences)\r\n \r\n-    if not user_sentences or not index:\r\n-        return jsonify({'overall_score': 0, 'flagged_sections': [], 'full_text_structured': []})\r\n+    if not user_sentences or not index or total_sentences == 0:\r\n+        # Return a complete empty report\r\n+        empty_stats = {\r\n+            \"total_sentences\": 0, \"direct_count\": 0, \"paraphrased_count\": 0, \"original_count\": 0,\r\n+            \"direct_percent\": 0, \"paraphrased_percent\": 0, \"original_percent\": 100\r\n+        }\r\n+        return jsonify({\r\n+            'overall_score': 0, \r\n+            'flagged_sections': [], \r\n+            'full_text_structured': [], \r\n+            'full_text': user_text,\r\n+            'stats': empty_stats\r\n+        })\r\n \r\n-    # (The rest of this function is identical to the one you have in app.py)\r\n-    DIRECT_THRESHOLD = 0.95\r\n-    PARAPHRASED_THRESHOLD = 0.75\r\n+    # --- THRESHOLDS FOR HYBRID ANALYSIS ---\r\n+    DIRECT_THRESHOLD = 0.95   # (95%+) For \"Direct Match\"\r\n+    PARAPHRASED_THRESHOLD = 0.75 # (75% - 94%) For \"Paraphrased\"\r\n \r\n+    # --- NEW: Initialize Counters ---\r\n+    direct_count = 0\r\n+    paraphrased_count = 0\r\n+    original_count = 0\r\n+    \r\n     report = {'overall_score': 0, 'flagged_sections': []}\r\n     full_text_structured = [] \r\n-    plagiarized_indices = set()\r\n     \r\n+    # --- FAISS-POWERED SEMANTIC SEARCH ---\r\n     user_embeddings = model.encode(user_sentences, convert_to_numpy=True).astype('float32')\r\n     faiss.normalize_L2(user_embeddings)\r\n \r\n     k = 1 \r\n     D, I = index.search(user_embeddings, k)\r\n \r\n-    for i in range(len(user_sentences)):\r\n+    for i in range(total_sentences):\r\n         similarity_score = D[i][0]\r\n         source_index = I[i][0]\r\n         current_sentence = user_sentences[i]\r\n         \r\n+        # Check against our thresholds\r\n         if similarity_score > DIRECT_THRESHOLD:\r\n             match_type = \"Direct Match\"\r\n             plagiarized = True\r\n+            direct_count += 1\r\n         elif similarity_score > PARAPHRASED_THRESHOLD:\r\n             match_type = \"Paraphrased\"\r\n             plagiarized = True\r\n+            paraphrased_count += 1\r\n         else:\r\n             match_type = \"Original\"\r\n             plagiarized = False\r\n+            original_count += 1\r\n         \r\n+        # Build the structured text array\r\n         if plagiarized:\r\n-            plagiarized_indices.add(i)\r\n             matched_sentence = source_sentences[source_index]\r\n             source_file, _ = source_metadata[source_index]\r\n             source_info = f\"{source_file} (similar to: \\\"{matched_sentence[0:100]}...\\\")\"\r\n             \r\n+            # Add to simple flagged sections list (for PDF report table)\r\n             report['flagged_sections'].append({\r\n                 'text': current_sentence,\r\n                 'source': source_info,\r\n                 'similarity': float(round(similarity_score * 100, 2)),\r\n                 'type': match_type\r\n             })\r\n             \r\n+            # Add to the new visual report structure\r\n             full_text_structured.append({\r\n                 \"text\": current_sentence,\r\n                 \"plagiarized\": True,\r\n                 \"type\": match_type,\r\n                 \"source\": source_info,\r\n                 \"similarity\": float(round(similarity_score * 100, 2))\r\n             })\r\n         else:\r\n+            # Add original sentences to the visual report structure\r\n             full_text_structured.append({\r\n                 \"text\": current_sentence,\r\n                 \"plagiarized\": False\r\n             })\r\n \r\n-    if len(user_sentences) > 0:\r\n-        report['overall_score'] = round((len(plagiarized_indices) / len(user_sentences)) * 100, 2)\r\n+    # --- NEW: Calculate Stats ---\r\n+    plagiarized_count = direct_count + paraphrased_count\r\n     \r\n+    stats = {\r\n+        \"total_sentences\": total_sentences,\r\n+        \"direct_count\": direct_count,\r\n+        \"paraphrased_count\": paraphrased_count,\r\n+        \"original_count\": original_count,\r\n+        \"direct_percent\": (direct_count / total_sentences) * 100,\r\n+        \"paraphrased_percent\": (paraphrased_count / total_sentences) * 100,\r\n+        \"original_percent\": (original_count / total_sentences) * 100\r\n+    }\r\n+\r\n+    # Add the new stats object to the report\r\n+    report['stats'] = stats\r\n+    \r\n+    report['overall_score'] = round((plagiarized_count / total_sentences) * 100, 2)\r\n     report['full_text_structured'] = full_text_structured\r\n     report['full_text'] = user_text \r\n     \r\n     return jsonify(report)\r\n \r\n \r\n @bp.route('/api/rewrite', methods=['POST'])\r\n def rewrite_sentence():\r\n-    # (This is the same fixed rewrite function from the bonus fix)\r\n+    # (This function is unchanged and correct)\r\n     data = request.get_json()\r\n     if not data or 'sentence' not in data:\r\n         return jsonify({'error': 'No sentence provided for rewrite'}), 400\r\n     if 'api_key' not in data or not data['api_key']:\r\n"
                }
            ],
            "date": 1762593757054,
            "name": "Commit-0",
            "content": "import requests\r\nimport numpy as np\r\nimport faiss\r\nfrom flask import request, jsonify, Blueprint\r\nfrom .loader import model, index, source_sentences, source_metadata\r\n\r\n# Create a 'Blueprint' - a collection of routes that can be registered with the app\r\nbp = Blueprint('main', __name__)\r\n\r\n@bp.route('/api/check', methods=['POST'])\r\ndef analyze_document():\r\n    data = request.get_json()\r\n    if not data or 'text' not in data:\r\n        return jsonify({'error': 'No text provided'}), 400\r\n\r\n    user_text = data['text']\r\n    user_sentences = nltk.sent_tokenize(user_text) # We need to import nltk in this file\r\n\r\n    if not user_sentences or not index:\r\n        return jsonify({'overall_score': 0, 'flagged_sections': [], 'full_text_structured': []})\r\n\r\n    # (The rest of this function is identical to the one you have in app.py)\r\n    DIRECT_THRESHOLD = 0.95\r\n    PARAPHRASED_THRESHOLD = 0.75\r\n\r\n    report = {'overall_score': 0, 'flagged_sections': []}\r\n    full_text_structured = [] \r\n    plagiarized_indices = set()\r\n    \r\n    user_embeddings = model.encode(user_sentences, convert_to_numpy=True).astype('float32')\r\n    faiss.normalize_L2(user_embeddings)\r\n\r\n    k = 1 \r\n    D, I = index.search(user_embeddings, k)\r\n\r\n    for i in range(len(user_sentences)):\r\n        similarity_score = D[i][0]\r\n        source_index = I[i][0]\r\n        current_sentence = user_sentences[i]\r\n        \r\n        if similarity_score > DIRECT_THRESHOLD:\r\n            match_type = \"Direct Match\"\r\n            plagiarized = True\r\n        elif similarity_score > PARAPHRASED_THRESHOLD:\r\n            match_type = \"Paraphrased\"\r\n            plagiarized = True\r\n        else:\r\n            match_type = \"Original\"\r\n            plagiarized = False\r\n        \r\n        if plagiarized:\r\n            plagiarized_indices.add(i)\r\n            matched_sentence = source_sentences[source_index]\r\n            source_file, _ = source_metadata[source_index]\r\n            source_info = f\"{source_file} (similar to: \\\"{matched_sentence[0:100]}...\\\")\"\r\n            \r\n            report['flagged_sections'].append({\r\n                'text': current_sentence,\r\n                'source': source_info,\r\n                'similarity': float(round(similarity_score * 100, 2)),\r\n                'type': match_type\r\n            })\r\n            \r\n            full_text_structured.append({\r\n                \"text\": current_sentence,\r\n                \"plagiarized\": True,\r\n                \"type\": match_type,\r\n                \"source\": source_info,\r\n                \"similarity\": float(round(similarity_score * 100, 2))\r\n            })\r\n        else:\r\n            full_text_structured.append({\r\n                \"text\": current_sentence,\r\n                \"plagiarized\": False\r\n            })\r\n\r\n    if len(user_sentences) > 0:\r\n        report['overall_score'] = round((len(plagiarized_indices) / len(user_sentences)) * 100, 2)\r\n    \r\n    report['full_text_structured'] = full_text_structured\r\n    report['full_text'] = user_text \r\n    \r\n    return jsonify(report)\r\n\r\n\r\n@bp.route('/api/rewrite', methods=['POST'])\r\ndef rewrite_sentence():\r\n    # (This is the same fixed rewrite function from the bonus fix)\r\n    data = request.get_json()\r\n    if not data or 'sentence' not in data:\r\n        return jsonify({'error': 'No sentence provided for rewrite'}), 400\r\n    if 'api_key' not in data or not data['api_key']:\r\n        return jsonify({'error': 'API key is missing from request'}), 400\r\n\r\n    sentence_to_rewrite = data['sentence']\r\n    api_key = data['api_key'] \r\n\r\n    prompt = f\"Rewrite the following sentence in a completely original way, maintaining the core meaning but using different vocabulary and structure: \\\"{sentence_to_rewrite}\\\"\"\r\n    gemini_api_url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent?key={api_key}\"\r\n    payload = {\"contents\": [{\"parts\": [{\"text\": prompt}]}]}\r\n\r\n    try:\r\n        response = requests.post(gemini_api_url, json=payload)\r\n        response.raise_for_status()\r\n        result = response.json()\r\n        \r\n        if 'error' in result:\r\n            print(f\"Gemini API returned an error: {result['error']['message']}\")\r\n            return jsonify({'error': result['error']['message']}), 500\r\n            \r\n        rewritten_text = result.get('candidates', [{}])[0].get('content', {}).get('parts', [{}])[0].get('text', 'Could not generate a suggestion.')\r\n        return jsonify({'rewritten_text': rewritten_text.strip()})\r\n        \r\n    except requests.exceptions.HTTPError as http_err:\r\n        try:\r\n            error_details = http_err.response.json().get('error', {}).get('message', str(http_err))\r\n        except:\r\n            error_details = str(http_err)\r\n        print(f\"HTTP error calling Gemini API: {error_details}\")\r\n        return jsonify({'error': error_details}), 500\r\n        \r\n    except Exception as e:\r\n        print(f\"Generic error in rewrite: {e}\")\r\n        return jsonify({'error': f'Failed to communicate with the rewrite service: {e}'}), 500"
        }
    ]
}