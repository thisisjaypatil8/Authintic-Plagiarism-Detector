{
    "sourceFile": "nlp-service/app.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 1,
            "patches": [
                {
                    "date": 1754056070913,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1754063465672,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -76,34 +76,34 @@\n         report['overall_score'] = round((len(plagiarized_indices) / len(user_sentences)) * 100, 2)\r\n \r\n     return jsonify(report)\r\n \r\n-# --- NEW GEMINI-POWERED REWRITE ENDPOINT ---\r\n+# --- GEMINI-POWERED REWRITE ENDPOINT ---\r\n @app.route('/rewrite', methods=['POST'])\r\n def rewrite_sentence():\r\n     data = request.get_json()\r\n+    # Check for both sentence and api_key from the backend request\r\n     if not data or 'sentence' not in data:\r\n         return jsonify({'error': 'No sentence provided for rewrite'}), 400\r\n+    if 'api_key' not in data or not data['api_key']:\r\n+        return jsonify({'error': 'API key is missing from request'}), 400\r\n \r\n     sentence_to_rewrite = data['sentence']\r\n-    \r\n-    # Construct the prompt for the Gemini API\r\n+    api_key = data['api_key'] # Use the key passed from the backend\r\n+\r\n     prompt = f\"Rewrite the following sentence in a completely original way, maintaining the core meaning but using different vocabulary and structure: \\\"{sentence_to_rewrite}\\\"\"\r\n     \r\n+    gemini_api_url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent?key={api_key}\"\r\n+\r\n     payload = {\r\n-        \"contents\": [{\r\n-            \"parts\": [{\r\n-                \"text\": prompt\r\n-            }]\r\n-        }]\r\n+        \"contents\": [{\"parts\": [{\"text\": prompt}]}]\r\n     }\r\n \r\n     try:\r\n-        response = requests.post(f\"{GEMINI_API_URL}{API_KEY}\", json=payload)\r\n-        response.raise_for_status() # Raise an exception for bad status codes\r\n+        response = requests.post(gemini_api_url, json=payload)\r\n+        response.raise_for_status()\r\n         result = response.json()\r\n         \r\n-        # Extract the rewritten text from the Gemini API response\r\n         rewritten_text = result.get('candidates', [{}])[0].get('content', {}).get('parts', [{}])[0].get('text', 'Could not generate a suggestion.')\r\n         \r\n         return jsonify({'rewritten_text': rewritten_text.strip()})\r\n     except requests.exceptions.RequestException as e:\r\n"
                }
            ],
            "date": 1754056070913,
            "name": "Commit-0",
            "content": "import os\r\nimport glob\r\nimport pickle\r\nimport torch\r\nimport nltk\r\nimport json\r\nimport requests\r\nfrom flask import Flask, request, jsonify\r\nfrom sentence_transformers import SentenceTransformer, util\r\nfrom sklearn.feature_extraction.text import TfidfVectorizer\r\nfrom sklearn.metrics.pairwise import cosine_similarity\r\n\r\napp = Flask(__name__)\r\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\r\n\r\n# --- LOAD PRE-PROCESSED DATA ---\r\ntry:\r\n    print(\"Loading pre-processed source embeddings...\")\r\n    source_embeddings = torch.load('source_embeddings.pt')\r\n    with open('source_data.pkl', 'rb') as f:\r\n        source_texts, source_names = pickle.load(f)\r\n    print(\"Data loaded successfully.\")\r\nexcept FileNotFoundError:\r\n    print(\"Error: Pre-processed data not found. Please run preprocess_sources.py first.\")\r\n    source_texts, source_names, source_embeddings = [], [], None\r\n\r\ntry:\r\n    nltk.data.find('tokenizers/punkt')\r\nexcept LookupError:\r\n    nltk.download('punkt')\r\n\r\n# --- GEMINI API SETUP ---\r\n# In a real deployed app, use an environment variable for the API key.\r\nGEMINI_API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent?key=\"\r\n# The API key will be left empty as per instructions, to be handled by the environment.\r\nAPI_KEY = \"\" \r\n\r\n@app.route('/analyze', methods=['POST'])\r\ndef analyze_document():\r\n    # This function remains the same as before\r\n    data = request.get_json()\r\n    if not data or 'text' not in data:\r\n        return jsonify({'error': 'No text provided'}), 400\r\n\r\n    user_text = data['text']\r\n    user_sentences = nltk.sent_tokenize(user_text)\r\n\r\n    report = {'overall_score': 0, 'flagged_sections': []}\r\n    plagiarized_indices = set()\r\n\r\n    for i, user_sentence in enumerate(user_sentences):\r\n        vectorizer = TfidfVectorizer().fit_transform([user_sentence] + source_texts)\r\n        syntactic_sims = cosine_similarity(vectorizer[0:1], vectorizer[1:])\r\n        \r\n        user_embedding = model.encode(user_sentence, convert_to_tensor=True)\r\n        semantic_sims = util.cos_sim(user_embedding, source_embeddings)\r\n        max_sim_score, max_sim_idx = torch.max(semantic_sims[0], dim=0)\r\n\r\n        if syntactic_sims.max() > 0.90:\r\n            source_idx = syntactic_sims.argmax()\r\n            plagiarized_indices.add(i)\r\n            report['flagged_sections'].append({\r\n                'text': user_sentence, 'source': source_names[source_idx],\r\n                'similarity': round(syntactic_sims.max() * 100, 2), 'type': 'Direct Match'\r\n            })\r\n            continue\r\n\r\n        if max_sim_score.item() > 0.75:\r\n            plagiarized_indices.add(i)\r\n            report['flagged_sections'].append({\r\n                'text': user_sentence, 'source': source_names[max_sim_idx.item()],\r\n                'similarity': round(max_sim_score.item() * 100, 2), 'type': 'Paraphrased'\r\n            })\r\n\r\n    if len(user_sentences) > 0:\r\n        report['overall_score'] = round((len(plagiarized_indices) / len(user_sentences)) * 100, 2)\r\n\r\n    return jsonify(report)\r\n\r\n# --- NEW GEMINI-POWERED REWRITE ENDPOINT ---\r\n@app.route('/rewrite', methods=['POST'])\r\ndef rewrite_sentence():\r\n    data = request.get_json()\r\n    if not data or 'sentence' not in data:\r\n        return jsonify({'error': 'No sentence provided for rewrite'}), 400\r\n\r\n    sentence_to_rewrite = data['sentence']\r\n    \r\n    # Construct the prompt for the Gemini API\r\n    prompt = f\"Rewrite the following sentence in a completely original way, maintaining the core meaning but using different vocabulary and structure: \\\"{sentence_to_rewrite}\\\"\"\r\n    \r\n    payload = {\r\n        \"contents\": [{\r\n            \"parts\": [{\r\n                \"text\": prompt\r\n            }]\r\n        }]\r\n    }\r\n\r\n    try:\r\n        response = requests.post(f\"{GEMINI_API_URL}{API_KEY}\", json=payload)\r\n        response.raise_for_status() # Raise an exception for bad status codes\r\n        result = response.json()\r\n        \r\n        # Extract the rewritten text from the Gemini API response\r\n        rewritten_text = result.get('candidates', [{}])[0].get('content', {}).get('parts', [{}])[0].get('text', 'Could not generate a suggestion.')\r\n        \r\n        return jsonify({'rewritten_text': rewritten_text.strip()})\r\n    except requests.exceptions.RequestException as e:\r\n        print(f\"Error calling Gemini API: {e}\")\r\n        return jsonify({'error': 'Failed to communicate with the rewrite service.'}), 500\r\n    except (KeyError, IndexError) as e:\r\n        print(f\"Error parsing Gemini API response: {e}\")\r\n        return jsonify({'error': 'Could not parse the suggestion.'}), 500\r\n\r\n\r\nif __name__ == '__main__':\r\n    app.run(debug=True, port=5001)"
        }
    ]
}