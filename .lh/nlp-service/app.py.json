{
    "sourceFile": "nlp-service/app.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 7,
            "patches": [
                {
                    "date": 1762431515986,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1762433380104,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,154 +1,143 @@\n # nlp-service/app.py\r\n \r\n import os\r\n import pickle\r\n-import torch\r\n-import nltk\r\n-import faiss\r\n import numpy as np\r\n-import requests\r\n from flask import Flask, request, jsonify\r\n from sentence_transformers import SentenceTransformer, util\r\n-from sklearn.feature_extraction.text import TfidfVectorizer\r\n-from sklearn.metrics.pairwise import cosine_similarity\r\n \r\n-app = Flask(__name__)\r\n-model = SentenceTransformer('all-MiniLM-L6-v2')\r\n-\r\n-# ---------------------------------------\r\n-# 1. LOAD PREPROCESSED DATA (FAISS + TF-IDF)\r\n-# ---------------------------------------\r\n+# --- Optional: use faiss if available ---\r\n try:\r\n-    print(\"Loading pre-processed source data...\")\r\n+    import faiss\r\n+except Exception:\r\n+    print(\"‚ö†Ô∏è FAISS not found ‚Äî using slower numpy fallback.\")\r\n+    class _LocalIndex:\r\n+        def __init__(self, dim):\r\n+            self.vectors = np.empty((0, dim), dtype='float32')\r\n \r\n-    # Load FAISS index for semantic similarity\r\n-    index = faiss.read_index('source_index.faiss')\r\n+        def add(self, x):\r\n+            self.vectors = np.vstack([self.vectors, np.asarray(x, dtype='float32')])\r\n \r\n-    # Load sentence embeddings metadata (for mapping)\r\n-    with open('source_data.pkl', 'rb') as f:\r\n-        source_texts, source_names = pickle.load(f)\r\n+        def search(self, query, k):\r\n+            sims = np.dot(self.vectors, query.T).squeeze()\r\n+            idx = np.argsort(-sims)[:k]\r\n+            return sims[idx].reshape(1, -1), idx.reshape(1, -1)\r\n \r\n-    # Pre-fit a global TF-IDF vectorizer for syntactic comparison\r\n-    tfidf_vectorizer = TfidfVectorizer().fit(source_texts)\r\n-    print(f\"‚úÖ Loaded {len(source_texts)} source sentences successfully.\")\r\n+        @property\r\n+        def ntotal(self):\r\n+            return len(self.vectors)\r\n \r\n-except Exception as e:\r\n-    print(f\"‚ùå FATAL: Failed to load preprocessed data: {e}\")\r\n-    print(\"Run `preprocess_sources.py` first.\")\r\n-    index, source_texts, source_names, tfidf_vectorizer = None, [], [], None\r\n+    class _FallbackFaiss:\r\n+        IndexFlatL2 = _LocalIndex\r\n+        read_index = lambda path: np.load(path, allow_pickle=True).item()\r\n \r\n-# Ensure NLTK tokenizer availability\r\n-try:\r\n-    nltk.data.find('tokenizers/punkt')\r\n-except LookupError:\r\n-    nltk.download('punkt')\r\n+    faiss = _FallbackFaiss()\r\n \r\n-# ---------------------------------------\r\n-# 2. ANALYZE DOCUMENT (Hybrid Sentence-Level)\r\n-# ---------------------------------------\r\n-@app.route('/analyze', methods=['POST'])\r\n-def analyze_document():\r\n-    if not source_texts or index is None:\r\n-        return jsonify({'error': 'Source data not loaded. Please run preprocess_sources.py.'}), 500\r\n+# --- Flask App ---\r\n+app = Flask(__name__)\r\n \r\n-    data = request.get_json()\r\n-    if not data or 'text' not in data or not data['text'].strip():\r\n-        return jsonify({'error': 'No text provided'}), 400\r\n+# --- Config ---\r\n+FAISS_INDEX_FILE = 'source_index.faiss'\r\n+DATA_FILE = 'source_data.pkl'\r\n+EMBEDDING_DIM = 384  # all-MiniLM-L6-v2\r\n+TOP_K = 5  # number of closest matches to check\r\n \r\n-    user_text = data['text']\r\n-    user_sentences = nltk.sent_tokenize(user_text)\r\n+# --- Load model ---\r\n+print(\"üîπ Loading SentenceTransformer model (all-MiniLM-L6-v2)...\")\r\n+model = SentenceTransformer('all-MiniLM-L6-v2')\r\n \r\n-    # Encode all user sentences for semantic similarity (once)\r\n-    user_embeddings = model.encode(user_sentences, convert_to_numpy=True).astype('float32')\r\n+# --- Load preprocessed data ---\r\n+try:\r\n+    print(\"üîπ Loading pre-processed source data...\")\r\n+    if not os.path.exists(FAISS_INDEX_FILE):\r\n+        raise FileNotFoundError(f\"{FAISS_INDEX_FILE} not found. Run preprocess_sources.py first.\")\r\n \r\n-    report = {\r\n-        'overall_score': 0,\r\n-        'document_visualizer': []\r\n-    }\r\n+    index = faiss.read_index(FAISS_INDEX_FILE)\r\n \r\n-    plagiarized_count = 0\r\n+    with open(DATA_FILE, 'rb') as f:\r\n+        source_names = pickle.load(f)\r\n \r\n-    # Search parameters\r\n-    k = 3  # top-k semantic neighbors per sentence\r\n-    SEMANTIC_THRESHOLD = 0.65\r\n-    SYNTACTIC_THRESHOLD = 0.90\r\n+    if len(source_names) != index.ntotal:\r\n+        print(f\"‚ö†Ô∏è Warning: Mismatch ‚Äî {len(source_names)} names vs {index.ntotal} vectors\")\r\n \r\n-    for i, sentence in enumerate(user_sentences):\r\n-        sentence_data = {'text': sentence, 'match': None}\r\n+    print(f\"‚úÖ Loaded FAISS index with {index.ntotal} documents.\")\r\n \r\n-        # --- Semantic Similarity via FAISS ---\r\n-        query_vec = user_embeddings[i].reshape(1, -1)\r\n-        D, I = index.search(query_vec, k)\r\n-        best_idx = I[0][0]\r\n-        l2_distance = D[0][0]\r\n+except Exception as e:\r\n+    print(f\"‚ùå FATAL: Failed to load preprocessed data: {e}\")\r\n+    print(\"Run `preprocess_sources.py` first.\")\r\n+    index = None\r\n+    source_names = []\r\n \r\n-        # Convert L2 distance to similarity score (0‚Äì1)\r\n-        semantic_score = max(0.0, 1.0 - (l2_distance / 2.0))\r\n+# --- Helper function for plagiarism check ---\r\n+def check_plagiarism(text):\r\n+    if not text.strip():\r\n+        return {\"overall_score\": 0, \"flagged_sections\": []}\r\n \r\n-        # --- Syntactic Similarity via TF-IDF ---\r\n-        vectorized = tfidf_vectorizer.transform([sentence])\r\n-        syntactic_scores = cosine_similarity(vectorized, tfidf_vectorizer.transform(source_texts))\r\n-        max_syntactic_score = syntactic_scores.max()\r\n-        syntactic_idx = syntactic_scores.argmax()\r\n+    sentences = [s.strip() for s in text.split('.') if s.strip()]\r\n+    sentence_embeddings = model.encode(sentences, convert_to_tensor=True)\r\n+    flagged_sections = []\r\n+    total_similarity = 0\r\n \r\n-        # --- Decision Logic ---\r\n-        if max_syntactic_score > SYNTACTIC_THRESHOLD:\r\n-            plagiarized_count += 1\r\n-            sentence_data['match'] = {\r\n-                'source': source_names[syntactic_idx],\r\n-                'similarity': round(max_syntactic_score * 100, 2),\r\n-                'type': 'Direct Match'\r\n-            }\r\n-        elif semantic_score > SEMANTIC_THRESHOLD:\r\n-            plagiarized_count += 1\r\n-            sentence_data['match'] = {\r\n-                'source': source_names[best_idx],\r\n-                'similarity': round(semantic_score * 100, 2),\r\n-                'type': 'Paraphrased'\r\n-            }\r\n+    for i, sentence_emb in enumerate(sentence_embeddings):\r\n+        query_vec = sentence_emb.cpu().numpy().reshape(1, -1)\r\n+        if index is None or index.ntotal == 0:\r\n+            continue\r\n+        distances, indices = index.search(query_vec, TOP_K)\r\n+        similarities = 1 - distances[0] / 2  # normalize to similarity\r\n+        best_idx = indices[0][0]\r\n+        similarity_score = float(similarities[0]) * 100\r\n+        total_similarity += similarity_score\r\n \r\n-        report['document_visualizer'].append(sentence_data)\r\n+        if similarity_score > 70:\r\n+            flagged_sections.append({\r\n+                \"text\": sentences[i],\r\n+                \"source\": source_names[best_idx] if best_idx < len(source_names) else \"Unknown\",\r\n+                \"similarity\": round(similarity_score, 2),\r\n+                \"type\": \"High Similarity\"\r\n+            })\r\n \r\n-    # Compute overall plagiarism percentage\r\n-    if len(user_sentences) > 0:\r\n-        report['overall_score'] = round((plagiarized_count / len(user_sentences)) * 100, 2)\r\n+    overall_score = round(total_similarity / max(len(sentences), 1), 2)\r\n+    return {\"overall_score\": overall_score, \"flagged_sections\": flagged_sections}\r\n \r\n-    return jsonify(report)\r\n+# --- Routes ---\r\n+@app.route('/api/check', methods=['POST'])\r\n+def analyze_text():\r\n+    try:\r\n+        data = request.get_json()\r\n+        text = data.get('text', '')\r\n+        result = check_plagiarism(text)\r\n+        return jsonify(result)\r\n+    except Exception as e:\r\n+        print(f\"‚ùå Error analyzing text: {e}\")\r\n+        return jsonify({\"error\": str(e)}), 500\r\n \r\n-# ---------------------------------------\r\n-# 3. GEMINI REWRITE ENDPOINT\r\n-# ---------------------------------------\r\n-@app.route('/rewrite', methods=['POST'])\r\n+\r\n+@app.route('/api/rewrite', methods=['POST'])\r\n def rewrite_sentence():\r\n-    data = request.get_json()\r\n-    if not data or 'sentence' not in data or 'api_key' not in data:\r\n-        return jsonify({'error': 'Missing sentence or API key'}), 400\r\n+    try:\r\n+        data = request.get_json()\r\n+        sentence = data.get('sentence', '')\r\n+        api_key = data.get('api_key', None)\r\n \r\n-    sentence_to_rewrite = data['sentence']\r\n-    api_key = data['api_key']\r\n+        if not sentence:\r\n+            return jsonify({\"error\": \"Sentence is required\"}), 400\r\n+        if not api_key:\r\n+            return jsonify({\"error\": \"Missing API key\"}), 403\r\n \r\n-    prompt = f\"Rewrite the following sentence in a completely original way, maintaining the core meaning but using different vocabulary and structure: \\\"{sentence_to_rewrite}\\\"\"\r\n-    gemini_api_url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent?key={api_key}\"\r\n+        # --- Simulated rewrite (you can replace this with Gemini or OpenAI API call) ---\r\n+        rewritten = sentence.replace(\" is \", \" can be \").replace(\" are \", \" could be \")\r\n+        rewritten = f\"Suggested rewrite: {rewritten}\"\r\n \r\n-    payload = {\"contents\": [{\"parts\": [{\"text\": prompt}]}]}\r\n-\r\n-    try:\r\n-        response = requests.post(gemini_api_url, json=payload)\r\n-        response.raise_for_status()\r\n-        result = response.json()\r\n-        rewritten_text = (\r\n-            result.get('candidates', [{}])[0]\r\n-            .get('content', {})\r\n-            .get('parts', [{}])[0]\r\n-            .get('text', 'Could not generate a suggestion.')\r\n-        )\r\n-        return jsonify({'rewritten_text': rewritten_text.strip()})\r\n+        return jsonify({\"rewritten_text\": rewritten})\r\n     except Exception as e:\r\n-        print(f\"Error calling Gemini API: {e}\")\r\n-        return jsonify({'error': 'Failed to get suggestion.'}), 500\r\n+        print(f\"‚ùå Rewrite error: {e}\")\r\n+        return jsonify({\"error\": str(e)}), 500\r\n \r\n-# ---------------------------------------\r\n-# 4. RUN SERVER\r\n-# ---------------------------------------\r\n+\r\n+@app.route('/', methods=['GET'])\r\n+def home():\r\n+    return jsonify({\"message\": \"NLP service running successfully\"})\r\n+\r\n+\r\n if __name__ == '__main__':\r\n-    app.run(debug=True, port=5001)\r\n+    app.run(host='127.0.0.1', port=5001, debug=True)\r\n"
                },
                {
                    "date": 1762437309870,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -99,9 +99,9 @@\n     overall_score = round(total_similarity / max(len(sentences), 1), 2)\r\n     return {\"overall_score\": overall_score, \"flagged_sections\": flagged_sections}\r\n \r\n # --- Routes ---\r\n-@app.route('/api/check', methods=['POST'])\r\n+@app.route('/api/chec', methods=['POST'])\r\n def analyze_text():\r\n     try:\r\n         data = request.get_json()\r\n         text = data.get('text', '')\r\n"
                },
                {
                    "date": 1762437317459,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -99,9 +99,9 @@\n     overall_score = round(total_similarity / max(len(sentences), 1), 2)\r\n     return {\"overall_score\": overall_score, \"flagged_sections\": flagged_sections}\r\n \r\n # --- Routes ---\r\n-@app.route('/api/chec', methods=['POST'])\r\n+@app.route('/analyze', methods=['POST'])\r\n def analyze_text():\r\n     try:\r\n         data = request.get_json()\r\n         text = data.get('text', '')\r\n"
                },
                {
                    "date": 1762441018597,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -2,38 +2,44 @@\n \r\n import os\r\n import pickle\r\n import numpy as np\r\n+import nltk\r\n from flask import Flask, request, jsonify\r\n-from sentence_transformers import SentenceTransformer, util\r\n+from sentence_transformers import SentenceTransformer\r\n+import google.generativeai as genai\r\n \r\n # --- Optional: use faiss if available ---\r\n try:\r\n     import faiss\r\n except Exception:\r\n     print(\"‚ö†Ô∏è FAISS not found ‚Äî using slower numpy fallback.\")\r\n+    # (Fallback classes remain the same)\r\n     class _LocalIndex:\r\n         def __init__(self, dim):\r\n             self.vectors = np.empty((0, dim), dtype='float32')\r\n-\r\n         def add(self, x):\r\n             self.vectors = np.vstack([self.vectors, np.asarray(x, dtype='float32')])\r\n-\r\n         def search(self, query, k):\r\n             sims = np.dot(self.vectors, query.T).squeeze()\r\n             idx = np.argsort(-sims)[:k]\r\n-            return sims[idx].reshape(1, -1), idx.reshape(1, -1)\r\n-\r\n+            # FAISS returns L2 distance, not similarity. We simulate this.\r\n+            # Low distance = high similarity.\r\n+            dists = 1 - sims[idx] \r\n+            return dists.reshape(1, -1), idx.reshape(1, -1)\r\n         @property\r\n         def ntotal(self):\r\n             return len(self.vectors)\r\n-\r\n     class _FallbackFaiss:\r\n         IndexFlatL2 = _LocalIndex\r\n-        read_index = lambda path: np.load(path, allow_pickle=True).item()\r\n-\r\n+        @staticmethod\r\n+        def read_index(path):\r\n+            idx = _LocalIndex(EMBEDDING_DIM)\r\n+            idx.vectors = np.load(path + '.npy')\r\n+            return idx\r\n     faiss = _FallbackFaiss()\r\n \r\n+\r\n # --- Flask App ---\r\n app = Flask(__name__)\r\n \r\n # --- Config ---\r\n@@ -41,16 +47,20 @@\n DATA_FILE = 'source_data.pkl'\r\n EMBEDDING_DIM = 384  # all-MiniLM-L6-v2\r\n TOP_K = 5  # number of closest matches to check\r\n \r\n+# --- NEW: Similarity Thresholds ---\r\n+DIRECT_THRESHOLD = 90.0  # (90%+) Anything this high is a copy/paste\r\n+PARAPHRASED_THRESHOLD = 70.0 # (70% - 89.9%) High similarity, likely paraphrased\r\n+\r\n # --- Load model ---\r\n print(\"üîπ Loading SentenceTransformer model (all-MiniLM-L6-v2)...\")\r\n model = SentenceTransformer('all-MiniLM-L6-v2')\r\n \r\n # --- Load preprocessed data ---\r\n try:\r\n     print(\"üîπ Loading pre-processed source data...\")\r\n-    if not os.path.exists(FAISS_INDEX_FILE):\r\n+    if not (os.path.exists(FAISS_INDEX_FILE) or os.path.exists(FAISS_INDEX_FILE + '.npy')):\r\n         raise FileNotFoundError(f\"{FAISS_INDEX_FILE} not found. Run preprocess_sources.py first.\")\r\n \r\n     index = faiss.read_index(FAISS_INDEX_FILE)\r\n \r\n@@ -61,40 +71,69 @@\n         print(f\"‚ö†Ô∏è Warning: Mismatch ‚Äî {len(source_names)} names vs {index.ntotal} vectors\")\r\n \r\n     print(f\"‚úÖ Loaded FAISS index with {index.ntotal} documents.\")\r\n \r\n+    # --- NEW: Load NLTK data ---\r\n+    print(\"üîπ Loading NLTK sentence tokenizer...\")\r\n+    nltk.download('punkt', quiet=True)\r\n+    print(\"‚úÖ NLTK ready.\")\r\n+\r\n except Exception as e:\r\n     print(f\"‚ùå FATAL: Failed to load preprocessed data: {e}\")\r\n     print(\"Run `preprocess_sources.py` first.\")\r\n     index = None\r\n     source_names = []\r\n \r\n-# --- Helper function for plagiarism check ---\r\n+# --- Helper function for plagiarism check (IMPROVED) ---\r\n def check_plagiarism(text):\r\n     if not text.strip():\r\n         return {\"overall_score\": 0, \"flagged_sections\": []}\r\n \r\n-    sentences = [s.strip() for s in text.split('.') if s.strip()]\r\n+    try:\r\n+        # --- NEW: Use NLTK for better sentence splitting ---\r\n+        sentences = nltk.sent_tokenize(text)\r\n+        sentences = [s.strip() for s in sentences if s.strip()]\r\n+        if not sentences:\r\n+             return {\"overall_score\": 0, \"flagged_sections\": []}\r\n+    except Exception as e:\r\n+        print(f\"Error tokenizing text: {e}\")\r\n+        return {\"overall_score\": 0, \"flagged_sections\": []}\r\n+\r\n     sentence_embeddings = model.encode(sentences, convert_to_tensor=True)\r\n     flagged_sections = []\r\n     total_similarity = 0\r\n \r\n     for i, sentence_emb in enumerate(sentence_embeddings):\r\n         query_vec = sentence_emb.cpu().numpy().reshape(1, -1)\r\n         if index is None or index.ntotal == 0:\r\n             continue\r\n+        \r\n+        # FAISS search: returns L2 distances and indices\r\n         distances, indices = index.search(query_vec, TOP_K)\r\n-        similarities = 1 - distances[0] / 2  # normalize to similarity\r\n+        \r\n+        # Get the *best* match (lowest distance)\r\n         best_idx = indices[0][0]\r\n-        similarity_score = float(similarities[0]) * 100\r\n+        best_dist = distances[0][0]\r\n+        \r\n+        # --- NEW: Convert L2 distance to a 0-100 similarity score ---\r\n+        # This is an approximation. 1.0 is a good \"medium\" distance.\r\n+        similarity_score = max(0, 1 - (best_dist / 1.0)) * 100\r\n+        \r\n         total_similarity += similarity_score\r\n \r\n-        if similarity_score > 70:\r\n+        # --- NEW: Use granular thresholds ---\r\n+        flag_type = None\r\n+        if similarity_score >= DIRECT_THRESHOLD:\r\n+            flag_type = \"Direct Match\"\r\n+        elif similarity_score >= PARAPHRASED_THRESHOLD:\r\n+            flag_type = \"Paraphrased\"\r\n+\r\n+        if flag_type:\r\n             flagged_sections.append({\r\n                 \"text\": sentences[i],\r\n                 \"source\": source_names[best_idx] if best_idx < len(source_names) else \"Unknown\",\r\n                 \"similarity\": round(similarity_score, 2),\r\n-                \"type\": \"High Similarity\"\r\n+                \"type\": flag_type  # <-- NEW: Add the type here\r\n             })\r\n \r\n     overall_score = round(total_similarity / max(len(sentences), 1), 2)\r\n     return {\"overall_score\": overall_score, \"flagged_sections\": flagged_sections}\r\n@@ -116,20 +155,45 @@\n def rewrite_sentence():\r\n     try:\r\n         data = request.get_json()\r\n         sentence = data.get('sentence', '')\r\n-        api_key = data.get('api_key', None)\r\n+        api_key = data.get('api_key', None) # Passed from Node.js (if configured)\r\n \r\n-        if not sentence:\r\n-            return jsonify({\"error\": \"Sentence is required\"}), 400\r\n-        if not api_key:\r\n-            return jsonify({\"error\": \"Missing API key\"}), 403\r\n+        # --- NEW: Gemini Rewrite ---\r\n+        if api_key:\r\n+            try:\r\n+                genai.configure(api_key=api_key)\r\n+                # Use a fast and efficient model\r\n+                model = genai.GenerativeModel('gemini-2.5-flash-preview-09-2025')\r\n+                prompt = f\"Concisely rewrite the following sentence to avoid plagiarism, while preserving its core meaning:\\n\\nOriginal: \\\"{sentence}\\\"\\n\\nRewritten:\"\r\n+                \r\n+                response = model.generate_content(\r\n+                    prompt,\r\n+                    generation_config=genai.GenerationConfig(\r\n+                        temperature=0.7,\r\n+                        max_output_tokens=200\r\n+                    )\r\n+                )\r\n+                rewritten = response.text.strip()\r\n+                \r\n+                # Simple cleanup in case model adds quotes\r\n+                if rewritten.startswith('\"') and rewritten.endswith('\"'):\r\n+                    rewritten = rewritten[1:-1]\r\n+                    \r\n+                return jsonify({\"rewritten_text\": rewritten})\r\n \r\n-        # --- Simulated rewrite (you can replace this with Gemini or OpenAI API call) ---\r\n+            except Exception as e:\r\n+                print(f\"‚ùå Gemini API error: {e}\")\r\n+                # Fallback to stub if API fails\r\n+                rewritten = f\"(API Error) Suggested: {sentence.replace(' is ', ' can be ')}\"\r\n+                return jsonify({\"rewritten_text\": rewritten})\r\n+\r\n+        # --- Fallback stub if no API key ---\r\n         rewritten = sentence.replace(\" is \", \" can be \").replace(\" are \", \" could be \")\r\n-        rewritten = f\"Suggested rewrite: {rewritten}\"\r\n-\r\n+        rewritten = f\"Suggested (stub): {rewritten}\"\r\n+        \r\n         return jsonify({\"rewritten_text\": rewritten})\r\n+        \r\n     except Exception as e:\r\n         print(f\"‚ùå Rewrite error: {e}\")\r\n         return jsonify({\"error\": str(e)}), 500\r\n \r\n@@ -139,5 +203,5 @@\n     return jsonify({\"message\": \"NLP service running successfully\"})\r\n \r\n \r\n if __name__ == '__main__':\r\n-    app.run(host='127.0.0.1', port=5001, debug=True)\r\n+    app.run(host='127.0.0.1', port=5001, debug=True)\n\\ No newline at end of file\n"
                },
                {
                    "date": 1762450494031,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,207 +1,121 @@\n-# nlp-service/app.py\r\n-\r\n import os\r\n import pickle\r\n+import torch\r\n+import nltk\r\n+import json\r\n+import requests\r\n+import faiss\r\n import numpy as np\r\n-import nltk\r\n+import hashlib\r\n from flask import Flask, request, jsonify\r\n from sentence_transformers import SentenceTransformer\r\n-import google.generativeai as genai\r\n-\r\n-# --- Optional: use faiss if available ---\r\n-try:\r\n-    import faiss\r\n-except Exception:\r\n-    print(\"‚ö†Ô∏è FAISS not found ‚Äî using slower numpy fallback.\")\r\n-    # (Fallback classes remain the same)\r\n-    class _LocalIndex:\r\n-        def __init__(self, dim):\r\n-            self.vectors = np.empty((0, dim), dtype='float32')\r\n-        def add(self, x):\r\n-            self.vectors = np.vstack([self.vectors, np.asarray(x, dtype='float32')])\r\n-        def search(self, query, k):\r\n-            sims = np.dot(self.vectors, query.T).squeeze()\r\n-            idx = np.argsort(-sims)[:k]\r\n-            # FAISS returns L2 distance, not similarity. We simulate this.\r\n-            # Low distance = high similarity.\r\n-            dists = 1 - sims[idx] \r\n-            return dists.reshape(1, -1), idx.reshape(1, -1)\r\n-        @property\r\n-        def ntotal(self):\r\n-            return len(self.vectors)\r\n-    class _FallbackFaiss:\r\n-        IndexFlatL2 = _LocalIndex\r\n-        @staticmethod\r\n-        def read_index(path):\r\n-            idx = _LocalIndex(EMBEDDING_DIM)\r\n-            idx.vectors = np.load(path + '.npy')\r\n-            return idx\r\n-    faiss = _FallbackFaiss()\r\n-\r\n-\r\n-# --- Flask App ---\r\n app = Flask(__name__)\r\n-\r\n-# --- Config ---\r\n-FAISS_INDEX_FILE = 'source_index.faiss'\r\n-DATA_FILE = 'source_data.pkl'\r\n-EMBEDDING_DIM = 384  # all-MiniLM-L6-v2\r\n-TOP_K = 5  # number of closest matches to check\r\n-\r\n-# --- NEW: Similarity Thresholds ---\r\n-DIRECT_THRESHOLD = 90.0  # (90%+) Anything this high is a copy/paste\r\n-PARAPHRASED_THRESHOLD = 70.0 # (70% - 89.9%) High similarity, likely paraphrased\r\n-\r\n-# --- Load model ---\r\n-print(\"üîπ Loading SentenceTransformer model (all-MiniLM-L6-v2)...\")\r\n model = SentenceTransformer('all-MiniLM-L6-v2')\r\n-\r\n-# --- Load preprocessed data ---\r\n+# --- 1. LOAD PRE-PROCESSED DOCUMENT DATA ---\r\n try:\r\n-    print(\"üîπ Loading pre-processed source data...\")\r\n-    if not (os.path.exists(FAISS_INDEX_FILE) or os.path.exists(FAISS_INDEX_FILE + '.npy')):\r\n-        raise FileNotFoundError(f\"{FAISS_INDEX_FILE} not found. Run preprocess_sources.py first.\")\r\n-\r\n-    index = faiss.read_index(FAISS_INDEX_FILE)\r\n-\r\n-    with open(DATA_FILE, 'rb') as f:\r\n-        source_names = pickle.load(f)\r\n-\r\n-    if len(source_names) != index.ntotal:\r\n-        print(f\"‚ö†Ô∏è Warning: Mismatch ‚Äî {len(source_names)} names vs {index.ntotal} vectors\")\r\n-\r\n-    print(f\"‚úÖ Loaded FAISS index with {index.ntotal} documents.\")\r\n-\r\n-    # --- NEW: Load NLTK data ---\r\n-    print(\"üîπ Loading NLTK sentence tokenizer...\")\r\n-    nltk.download('punkt', quiet=True)\r\n-    print(\"‚úÖ NLTK ready.\")\r\n-\r\n+    print(\"Loading pre-processed document embeddings and FAISS index...\")\r\n+    index = faiss.read_index('source_index.faiss')\r\n+    \r\n+    # Load the source FILENAMES\r\n+    with open('source_data.pkl', 'rb') as f:\r\n+        # This now correctly loads the list of names from your preprocess script\r\n+        source_names = pickle.load(f) \r\n+    print(\"Data loaded successfully.\")\r\n+    \r\n except Exception as e:\r\n-    print(f\"‚ùå FATAL: Failed to load preprocessed data: {e}\")\r\n-    print(\"Run `preprocess_sources.py` first.\")\r\n-    index = None\r\n-    source_names = []\r\n-\r\n-# --- Helper function for plagiarism check (IMPROVED) ---\r\n-def check_plagiarism(text):\r\n-    if not text.strip():\r\n-        return {\"overall_score\": 0, \"flagged_sections\": []}\r\n-\r\n+    print(f\"Error loading preprocessed data: {e}\")\r\n+    print(\"Please STOP the server, run preprocess_sources.py, then restart.\")\r\n+    index, source_names = None, []\r\n+# NLTK Tokenizer (still needed for rewrite)\r\n+try:\r\n+    nltk.data.find('tokenizers/punkt')\r\n+except LookupError:\r\n+    nltk.download('punkt')\r\n+# --- 2. CORE ANALYSIS ROUTE (Compares Full Document) ---\r\n+@app.route('/api/check', methods=['POST'])\r\n+def analyze_document():\r\n+    data = request.get_json()\r\n+    if not data or 'text' not in data:\r\n+        return jsonify({'error': 'No text provided'}), 400\r\n+    user_text = data['text']\r\n+    \r\n+    if not user_text or not index:\r\n+        return jsonify({'overall_score': 0, 'flagged_sections': []})\r\n+    # --- 1. ENCODE THE ENTIRE USER DOCUMENT ---\r\n+    # We compare the user's full text to the full source texts\r\n     try:\r\n-        # --- NEW: Use NLTK for better sentence splitting ---\r\n-        sentences = nltk.sent_tokenize(text)\r\n-        sentences = [s.strip() for s in sentences if s.strip()]\r\n-        if not sentences:\r\n-             return {\"overall_score\": 0, \"flagged_sections\": []}\r\n+        user_embedding = model.encode(user_text, convert_to_numpy=True).astype('float32')\r\n     except Exception as e:\r\n-        print(f\"Error tokenizing text: {e}\")\r\n-        return {\"overall_score\": 0, \"flagged_sections\": []}\r\n-\r\n-    sentence_embeddings = model.encode(sentences, convert_to_tensor=True)\r\n-    flagged_sections = []\r\n-    total_similarity = 0\r\n-\r\n-    for i, sentence_emb in enumerate(sentence_embeddings):\r\n-        query_vec = sentence_emb.cpu().numpy().reshape(1, -1)\r\n-        if index is None or index.ntotal == 0:\r\n-            continue\r\n+        print(f\"Error encoding user text: {e}\")\r\n+        return jsonify({'error': 'Failed to process text.'}), 500\r\n         \r\n-        # FAISS search: returns L2 distances and indices\r\n-        distances, indices = index.search(query_vec, TOP_K)\r\n+    user_embedding = user_embedding.reshape(1, -1) # Make it a (1, D) array\r\n+    # --- 2. SEARCH THE FAISS (L2) INDEX ---\r\n+    # Your script uses IndexFlatL2, which finds the *closest* vector (lowest distance)\r\n+    # k=1 means find the single best match\r\n+    k = 1\r\n+    D, I = index.search(user_embedding, k)\r\n+    \r\n+    l2_distance = D[0][0]\r\n+    best_match_index = I[0][0]\r\n+    \r\n+    # --- 3. PREPARE THE REPORT ---\r\n+    report = {'overall_score': 0, 'flagged_sections': []}\r\n+    \r\n+    # With L2 distance, 0.0 is a perfect match.\r\n+    # A low distance (e.g., < 0.7) means it's very similar.\r\n+    L2_DISTANCE_THRESHOLD = 0.7 \r\n+    if l2_distance < L2_DISTANCE_THRESHOLD:\r\n+        # We found a significant match!\r\n         \r\n-        # Get the *best* match (lowest distance)\r\n-        best_idx = indices[0][0]\r\n-        best_dist = distances[0][0]\r\n+        # --- THIS IS THE FIX ---\r\n+        # Convert L2 distance to a 0-100% similarity score\r\n+        # We wrap in float() to fix the JSON serializable error\r\n+        similarity_percent = float(round((1.0 - (l2_distance / 2.0)) * 100, 2))\r\n+        # --- END OF FIX ---\r\n         \r\n-        # --- NEW: Convert L2 distance to a 0-100 similarity score ---\r\n-        # This is an approximation. 1.0 is a good \"medium\" distance.\r\n-        similarity_score = max(0, 1 - (best_dist / 1.0)) * 100\r\n+        # Get the filename of the match\r\n+        source_name = source_names[best_match_index] \r\n         \r\n-        total_similarity += similarity_score\r\n+        report['overall_score'] = similarity_percent\r\n+        \r\n+        # Since this is a full-document check, we create one report entry\r\n+        report['flagged_sections'].append({\r\n+            'text': f\"The entire document is {similarity_percent}% similar to a source file.\",\r\n+            'source': source_name,\r\n+            'similarity': similarity_percent,\r\n+            'type': 'Full Document Match'\r\n+        })\r\n+    \r\n+    return jsonify(report)\r\n \r\n-        # --- NEW: Use granular thresholds ---\r\n-        flag_type = None\r\n-        if similarity_score >= DIRECT_THRESHOLD:\r\n-            flag_type = \"Direct Match\"\r\n-        elif similarity_score >= PARAPHRASED_THRESHOLD:\r\n-            flag_type = \"Paraphrased\"\r\n-\r\n-        if flag_type:\r\n-            flagged_sections.append({\r\n-                \"text\": sentences[i],\r\n\\ No newline at end of file\n-                \"source\": source_names[best_idx] if best_idx < len(source_names) else \"Unknown\",\r\n-                \"similarity\": round(similarity_score, 2),\r\n-                \"type\": flag_type  # <-- NEW: Add the type here\r\n-            })\r\n-\r\n-    overall_score = round(total_similarity / max(len(sentences), 1), 2)\r\n-    return {\"overall_score\": overall_score, \"flagged_sections\": flagged_sections}\r\n-\r\n-# --- Routes ---\r\n-@app.route('/analyze', methods=['POST'])\r\n-def analyze_text():\r\n-    try:\r\n-        data = request.get_json()\r\n-        text = data.get('text', '')\r\n-        result = check_plagiarism(text)\r\n-        return jsonify(result)\r\n-    except Exception as e:\r\n-        print(f\"‚ùå Error analyzing text: {e}\")\r\n-        return jsonify({\"error\": str(e)}), 500\r\n-\r\n-\r\n+# --- 3. GEMINI-POWERED REWRITE ENDPOINT ---\r\n+# (This route was correct, leaving as-is)\r\n @app.route('/api/rewrite', methods=['POST'])\r\n def rewrite_sentence():\r\n+    data = request.get_json()\r\n+    if not data or 'sentence' not in data:\r\n+        return jsonify({'error': 'No sentence provided for rewrite'}), 400\r\n+    if 'api_key' not in data or not data['api_key']:\r\n+        return jsonify({'error': 'API key is missing from request'}), 400\r\n+    sentence_to_rewrite = data['sentence']\r\n+    api_key = data['api_key'] \r\n+    prompt = f\"Rewrite the following sentence in a completely original way, maintaining the core meaning but using different vocabulary and structure: \\\"{sentence_to_rewrite}\\\"\"\r\n+    \r\n+    gemini_api_url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent?key={api_key}\"\r\n+    payload = {\r\n+        \"contents\": [{\"parts\": [{\"text\": prompt}]}]\r\n+    }\r\n     try:\r\n-        data = request.get_json()\r\n-        sentence = data.get('sentence', '')\r\n-        api_key = data.get('api_key', None) # Passed from Node.js (if configured)\r\n-\r\n-        # --- NEW: Gemini Rewrite ---\r\n-        if api_key:\r\n-            try:\r\n-                genai.configure(api_key=api_key)\r\n-                # Use a fast and efficient model\r\n-                model = genai.GenerativeModel('gemini-2.5-flash-preview-09-2025')\r\n-                prompt = f\"Concisely rewrite the following sentence to avoid plagiarism, while preserving its core meaning:\\n\\nOriginal: \\\"{sentence}\\\"\\n\\nRewritten:\"\r\n-                \r\n-                response = model.generate_content(\r\n-                    prompt,\r\n-                    generation_config=genai.GenerationConfig(\r\n-                        temperature=0.7,\r\n-                        max_output_tokens=200\r\n-                    )\r\n-                )\r\n-                rewritten = response.text.strip()\r\n-                \r\n-                # Simple cleanup in case model adds quotes\r\n-                if rewritten.startswith('\"') and rewritten.endswith('\"'):\r\n-                    rewritten = rewritten[1:-1]\r\n-                    \r\n-                return jsonify({\"rewritten_text\": rewritten})\r\n-\r\n-            except Exception as e:\r\n-                print(f\"‚ùå Gemini API error: {e}\")\r\n-                # Fallback to stub if API fails\r\n-                rewritten = f\"(API Error) Suggested: {sentence.replace(' is ', ' can be ')}\"\r\n-                return jsonify({\"rewritten_text\": rewritten})\r\n-\r\n-        # --- Fallback stub if no API key ---\r\n-        rewritten = sentence.replace(\" is \", \" can be \").replace(\" are \", \" could be \")\r\n-        rewritten = f\"Suggested (stub): {rewritten}\"\r\n-        \r\n-        return jsonify({\"rewritten_text\": rewritten})\r\n-        \r\n+        response = requests.post(gemini_api_url, json=payload)\r\n+        response.raise_for_status()\r\n+        result = response.json()\r\n+        rewritten_text = result.get('candidates', [{}])[0].get('content', {}).get('parts', [{}])[0].get('text', 'Could not generate a suggestion.')\r\n+        return jsonify({'rewritten_text': rewritten_text.strip()})\r\n     except Exception as e:\r\n-        print(f\"‚ùå Rewrite error: {e}\")\r\n-        return jsonify({\"error\": str(e)}), 500\r\n+        print(f\"Error calling Gemini API: {e}\")\r\n+        return jsonify({'error': 'Failed to communicate with the rewrite service.'}), 500\r\n \r\n-\r\n-@app.route('/', methods=['GET'])\r\n-def home():\r\n-    return jsonify({\"message\": \"NLP service running successfully\"})\r\n-\r\n-\r\n+# --- 4. START THE SERVER ---\r\n if __name__ == '__main__':\r\n-    app.run(host='127.0.0.1', port=5001, debug=True)\n+    app.run(debug=True, port=5001)\r\n"
                },
                {
                    "date": 1762493462689,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -8,10 +8,12 @@\n import numpy as np\r\n import hashlib\r\n from flask import Flask, request, jsonify\r\n from sentence_transformers import SentenceTransformer\r\n+\r\n app = Flask(__name__)\r\n model = SentenceTransformer('all-MiniLM-L6-v2')\r\n+\r\n # --- 1. LOAD PRE-PROCESSED DOCUMENT DATA ---\r\n try:\r\n     print(\"Loading pre-processed document embeddings and FAISS index...\")\r\n     index = faiss.read_index('source_index.faiss')\r\n@@ -25,23 +27,27 @@\n except Exception as e:\r\n     print(f\"Error loading preprocessed data: {e}\")\r\n     print(\"Please STOP the server, run preprocess_sources.py, then restart.\")\r\n     index, source_names = None, []\r\n+\r\n # NLTK Tokenizer (still needed for rewrite)\r\n try:\r\n     nltk.data.find('tokenizers/punkt')\r\n except LookupError:\r\n     nltk.download('punkt')\r\n+\r\n # --- 2. CORE ANALYSIS ROUTE (Compares Full Document) ---\r\n @app.route('/api/check', methods=['POST'])\r\n def analyze_document():\r\n     data = request.get_json()\r\n     if not data or 'text' not in data:\r\n         return jsonify({'error': 'No text provided'}), 400\r\n+\r\n     user_text = data['text']\r\n     \r\n     if not user_text or not index:\r\n         return jsonify({'overall_score': 0, 'flagged_sections': []})\r\n+\r\n     # --- 1. ENCODE THE ENTIRE USER DOCUMENT ---\r\n     # We compare the user's full text to the full source texts\r\n     try:\r\n         user_embedding = model.encode(user_text, convert_to_numpy=True).astype('float32')\r\n@@ -49,8 +55,9 @@\n         print(f\"Error encoding user text: {e}\")\r\n         return jsonify({'error': 'Failed to process text.'}), 500\r\n         \r\n     user_embedding = user_embedding.reshape(1, -1) # Make it a (1, D) array\r\n+\r\n     # --- 2. SEARCH THE FAISS (L2) INDEX ---\r\n     # Your script uses IndexFlatL2, which finds the *closest* vector (lowest distance)\r\n     # k=1 means find the single best match\r\n     k = 1\r\n@@ -64,8 +71,9 @@\n     \r\n     # With L2 distance, 0.0 is a perfect match.\r\n     # A low distance (e.g., < 0.7) means it's very similar.\r\n     L2_DISTANCE_THRESHOLD = 0.7 \r\n+\r\n     if l2_distance < L2_DISTANCE_THRESHOLD:\r\n         # We found a significant match!\r\n         \r\n         # --- THIS IS THE FIX ---\r\n@@ -88,8 +96,9 @@\n         })\r\n     \r\n     return jsonify(report)\r\n \r\n+\r\n # --- 3. GEMINI-POWERED REWRITE ENDPOINT ---\r\n # (This route was correct, leaving as-is)\r\n @app.route('/api/rewrite', methods=['POST'])\r\n def rewrite_sentence():\r\n@@ -97,16 +106,21 @@\n     if not data or 'sentence' not in data:\r\n         return jsonify({'error': 'No sentence provided for rewrite'}), 400\r\n     if 'api_key' not in data or not data['api_key']:\r\n         return jsonify({'error': 'API key is missing from request'}), 400\r\n+\r\n+\r\n     sentence_to_rewrite = data['sentence']\r\n     api_key = data['api_key'] \r\n+\r\n     prompt = f\"Rewrite the following sentence in a completely original way, maintaining the core meaning but using different vocabulary and structure: \\\"{sentence_to_rewrite}\\\"\"\r\n     \r\n     gemini_api_url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent?key={api_key}\"\r\n+\r\n     payload = {\r\n         \"contents\": [{\"parts\": [{\"text\": prompt}]}]\r\n     }\r\n+\r\n     try:\r\n         response = requests.post(gemini_api_url, json=payload)\r\n         response.raise_for_status()\r\n         result = response.json()\r\n@@ -115,7 +129,8 @@\n     except Exception as e:\r\n         print(f\"Error calling Gemini API: {e}\")\r\n         return jsonify({'error': 'Failed to communicate with the rewrite service.'}), 500\r\n \r\n+\r\n # --- 4. START THE SERVER ---\r\n if __name__ == '__main__':\r\n-    app.run(debug=True, port=5001)\r\n+    app.run(debug=True, port=5001)\n\\ No newline at end of file\n"
                },
                {
                    "date": 1762525070557,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -12,115 +12,96 @@\n \r\n app = Flask(__name__)\r\n model = SentenceTransformer('all-MiniLM-L6-v2')\r\n \r\n-# --- 1. LOAD PRE-PROCESSED DOCUMENT DATA ---\r\n+# --- 1. LOAD PRE-PROCESSED SENTENCE DATA ---\r\n try:\r\n-    print(\"Loading pre-processed document embeddings and FAISS index...\")\r\n+    print(\"Loading pre-processed sentence embeddings and FAISS index...\")\r\n     index = faiss.read_index('source_index.faiss')\r\n-    \r\n-    # Load the source FILENAMES\r\n+\r\n     with open('source_data.pkl', 'rb') as f:\r\n-        # This now correctly loads the list of names from your preprocess script\r\n-        source_names = pickle.load(f) \r\n+        source_sentences, source_metadata = pickle.load(f)\r\n     print(\"Data loaded successfully.\")\r\n-    \r\n+\r\n except Exception as e:\r\n     print(f\"Error loading preprocessed data: {e}\")\r\n     print(\"Please STOP the server, run preprocess_sources.py, then restart.\")\r\n-    index, source_names = None, []\r\n+    index, source_sentences, source_metadata = None, [], []\r\n \r\n-# NLTK Tokenizer (still needed for rewrite)\r\n+# NLTK Tokenizer\r\n try:\r\n     nltk.data.find('tokenizers/punkt')\r\n except LookupError:\r\n     nltk.download('punkt')\r\n \r\n-# --- 2. CORE ANALYSIS ROUTE (Compares Full Document) ---\r\n+# --- 2. CORE ANALYSIS ROUTE (NOW USES FAISS) ---\r\n @app.route('/api/check', methods=['POST'])\r\n def analyze_document():\r\n     data = request.get_json()\r\n     if not data or 'text' not in data:\r\n         return jsonify({'error': 'No text provided'}), 400\r\n \r\n     user_text = data['text']\r\n-    \r\n-    if not user_text or not index:\r\n-        return jsonify({'overall_score': 0, 'flagged_sections': []})\r\n+    user_sentences = nltk.sent_tokenize(user_text)\r\n \r\n-    # --- 1. ENCODE THE ENTIRE USER DOCUMENT ---\r\n-    # We compare the user's full text to the full source texts\r\n-    try:\r\n-        user_embedding = model.encode(user_text, convert_to_numpy=True).astype('float32')\r\n-    except Exception as e:\r\n-        print(f\"Error encoding user text: {e}\")\r\n-        return jsonify({'error': 'Failed to process text.'}), 500\r\n-        \r\n-    user_embedding = user_embedding.reshape(1, -1) # Make it a (1, D) array\r\n+    if not user_sentences or not index:\r\n+        return jsonify({'overall_score': 0, 'flagged_sections': [], 'full_text': user_text})\r\n \r\n-    # --- 2. SEARCH THE FAISS (L2) INDEX ---\r\n-    # Your script uses IndexFlatL2, which finds the *closest* vector (lowest distance)\r\n-    # k=1 means find the single best match\r\n-    k = 1\r\n-    D, I = index.search(user_embedding, k)\r\n-    \r\n-    l2_distance = D[0][0]\r\n-    best_match_index = I[0][0]\r\n-    \r\n-    # --- 3. PREPARE THE REPORT ---\r\n     report = {'overall_score': 0, 'flagged_sections': []}\r\n-    \r\n-    # With L2 distance, 0.0 is a perfect match.\r\n-    # A low distance (e.g., < 0.7) means it's very similar.\r\n-    L2_DISTANCE_THRESHOLD = 0.7 \r\n+    plagiarized_indices = set()\r\n \r\n-    if l2_distance < L2_DISTANCE_THRESHOLD:\r\n-        # We found a significant match!\r\n-        \r\n-        # --- THIS IS THE FIX ---\r\n-        # Convert L2 distance to a 0-100% similarity score\r\n-        # We wrap in float() to fix the JSON serializable error\r\n-        similarity_percent = float(round((1.0 - (l2_distance / 2.0)) * 100, 2))\r\n-        # --- END OF FIX ---\r\n-        \r\n-        # Get the filename of the match\r\n-        source_name = source_names[best_match_index] \r\n-        \r\n-        report['overall_score'] = similarity_percent\r\n-        \r\n-        # Since this is a full-document check, we create one report entry\r\n-        report['flagged_sections'].append({\r\n-            'text': f\"The entire document is {similarity_percent}% similar to a source file.\",\r\n-            'source': source_name,\r\n-            'similarity': similarity_percent,\r\n-            'type': 'Full Document Match'\r\n-        })\r\n-    \r\n+    # --- FAISS-POWERED SEMANTIC SEARCH ---\r\n+    user_embeddings = model.encode(user_sentences, convert_to_numpy=True).astype('float32')\r\n+    faiss.normalize_L2(user_embeddings)\r\n+\r\n+    k = 1 # Find the single best match for each sentence\r\n+    D, I = index.search(user_embeddings, k)\r\n+\r\n+    SIMILARITY_THRESHOLD = 0.75 # Tune this threshold (0.75 is a good start)\r\n+\r\n+    for i in range(len(user_sentences)):\r\n+        similarity_score = D[i][0]\r\n+        source_index = I[i][0]\r\n+\r\n+        if similarity_score > SIMILARITY_THRESHOLD:\r\n+            plagiarized_indices.add(i)\r\n+\r\n+            matched_sentence = source_sentences[source_index]\r\n+            source_file, _ = source_metadata[source_index]\r\n+\r\n+            report['flagged_sections'].append({\r\n+                'text': user_sentences[i],\r\n+                'source': f\"{source_file} (similar to: \\\"{matched_sentence[0:100]}...\\\")\",\r\n+                'similarity': float(round(similarity_score * 100, 2)),\r\n+                'type': 'Paraphrased' \r\n+            })\r\n+\r\n+    if len(user_sentences) > 0:\r\n+        report['overall_score'] = round((len(plagiarized_indices) / len(user_sentences)) * 100, 2)\r\n+\r\n+    # IMPORTANT: Send the original text back, we need it for the PDF report\r\n+    report['full_text'] = user_text \r\n+\r\n     return jsonify(report)\r\n \r\n \r\n # --- 3. GEMINI-POWERED REWRITE ENDPOINT ---\r\n-# (This route was correct, leaving as-is)\r\n @app.route('/api/rewrite', methods=['POST'])\r\n def rewrite_sentence():\r\n     data = request.get_json()\r\n     if not data or 'sentence' not in data:\r\n         return jsonify({'error': 'No sentence provided for rewrite'}), 400\r\n     if 'api_key' not in data or not data['api_key']:\r\n         return jsonify({'error': 'API key is missing from request'}), 400\r\n \r\n-\r\n     sentence_to_rewrite = data['sentence']\r\n     api_key = data['api_key'] \r\n \r\n     prompt = f\"Rewrite the following sentence in a completely original way, maintaining the core meaning but using different vocabulary and structure: \\\"{sentence_to_rewrite}\\\"\"\r\n-    \r\n+\r\n     gemini_api_url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent?key={api_key}\"\r\n+    payload = {\"contents\": [{\"parts\": [{\"text\": prompt}]}]}\r\n \r\n-    payload = {\r\n-        \"contents\": [{\"parts\": [{\"text\": prompt}]}]\r\n-    }\r\n-\r\n     try:\r\n         response = requests.post(gemini_api_url, json=payload)\r\n         response.raise_for_status()\r\n         result = response.json()\r\n"
                }
            ],
            "date": 1762431515985,
            "name": "Commit-0",
            "content": "# nlp-service/app.py\r\n\r\nimport os\r\nimport pickle\r\nimport torch\r\nimport nltk\r\nimport faiss\r\nimport numpy as np\r\nimport requests\r\nfrom flask import Flask, request, jsonify\r\nfrom sentence_transformers import SentenceTransformer, util\r\nfrom sklearn.feature_extraction.text import TfidfVectorizer\r\nfrom sklearn.metrics.pairwise import cosine_similarity\r\n\r\napp = Flask(__name__)\r\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\r\n\r\n# ---------------------------------------\r\n# 1. LOAD PREPROCESSED DATA (FAISS + TF-IDF)\r\n# ---------------------------------------\r\ntry:\r\n    print(\"Loading pre-processed source data...\")\r\n\r\n    # Load FAISS index for semantic similarity\r\n    index = faiss.read_index('source_index.faiss')\r\n\r\n    # Load sentence embeddings metadata (for mapping)\r\n    with open('source_data.pkl', 'rb') as f:\r\n        source_texts, source_names = pickle.load(f)\r\n\r\n    # Pre-fit a global TF-IDF vectorizer for syntactic comparison\r\n    tfidf_vectorizer = TfidfVectorizer().fit(source_texts)\r\n    print(f\"‚úÖ Loaded {len(source_texts)} source sentences successfully.\")\r\n\r\nexcept Exception as e:\r\n    print(f\"‚ùå FATAL: Failed to load preprocessed data: {e}\")\r\n    print(\"Run `preprocess_sources.py` first.\")\r\n    index, source_texts, source_names, tfidf_vectorizer = None, [], [], None\r\n\r\n# Ensure NLTK tokenizer availability\r\ntry:\r\n    nltk.data.find('tokenizers/punkt')\r\nexcept LookupError:\r\n    nltk.download('punkt')\r\n\r\n# ---------------------------------------\r\n# 2. ANALYZE DOCUMENT (Hybrid Sentence-Level)\r\n# ---------------------------------------\r\n@app.route('/analyze', methods=['POST'])\r\ndef analyze_document():\r\n    if not source_texts or index is None:\r\n        return jsonify({'error': 'Source data not loaded. Please run preprocess_sources.py.'}), 500\r\n\r\n    data = request.get_json()\r\n    if not data or 'text' not in data or not data['text'].strip():\r\n        return jsonify({'error': 'No text provided'}), 400\r\n\r\n    user_text = data['text']\r\n    user_sentences = nltk.sent_tokenize(user_text)\r\n\r\n    # Encode all user sentences for semantic similarity (once)\r\n    user_embeddings = model.encode(user_sentences, convert_to_numpy=True).astype('float32')\r\n\r\n    report = {\r\n        'overall_score': 0,\r\n        'document_visualizer': []\r\n    }\r\n\r\n    plagiarized_count = 0\r\n\r\n    # Search parameters\r\n    k = 3  # top-k semantic neighbors per sentence\r\n    SEMANTIC_THRESHOLD = 0.65\r\n    SYNTACTIC_THRESHOLD = 0.90\r\n\r\n    for i, sentence in enumerate(user_sentences):\r\n        sentence_data = {'text': sentence, 'match': None}\r\n\r\n        # --- Semantic Similarity via FAISS ---\r\n        query_vec = user_embeddings[i].reshape(1, -1)\r\n        D, I = index.search(query_vec, k)\r\n        best_idx = I[0][0]\r\n        l2_distance = D[0][0]\r\n\r\n        # Convert L2 distance to similarity score (0‚Äì1)\r\n        semantic_score = max(0.0, 1.0 - (l2_distance / 2.0))\r\n\r\n        # --- Syntactic Similarity via TF-IDF ---\r\n        vectorized = tfidf_vectorizer.transform([sentence])\r\n        syntactic_scores = cosine_similarity(vectorized, tfidf_vectorizer.transform(source_texts))\r\n        max_syntactic_score = syntactic_scores.max()\r\n        syntactic_idx = syntactic_scores.argmax()\r\n\r\n        # --- Decision Logic ---\r\n        if max_syntactic_score > SYNTACTIC_THRESHOLD:\r\n            plagiarized_count += 1\r\n            sentence_data['match'] = {\r\n                'source': source_names[syntactic_idx],\r\n                'similarity': round(max_syntactic_score * 100, 2),\r\n                'type': 'Direct Match'\r\n            }\r\n        elif semantic_score > SEMANTIC_THRESHOLD:\r\n            plagiarized_count += 1\r\n            sentence_data['match'] = {\r\n                'source': source_names[best_idx],\r\n                'similarity': round(semantic_score * 100, 2),\r\n                'type': 'Paraphrased'\r\n            }\r\n\r\n        report['document_visualizer'].append(sentence_data)\r\n\r\n    # Compute overall plagiarism percentage\r\n    if len(user_sentences) > 0:\r\n        report['overall_score'] = round((plagiarized_count / len(user_sentences)) * 100, 2)\r\n\r\n    return jsonify(report)\r\n\r\n# ---------------------------------------\r\n# 3. GEMINI REWRITE ENDPOINT\r\n# ---------------------------------------\r\n@app.route('/rewrite', methods=['POST'])\r\ndef rewrite_sentence():\r\n    data = request.get_json()\r\n    if not data or 'sentence' not in data or 'api_key' not in data:\r\n        return jsonify({'error': 'Missing sentence or API key'}), 400\r\n\r\n    sentence_to_rewrite = data['sentence']\r\n    api_key = data['api_key']\r\n\r\n    prompt = f\"Rewrite the following sentence in a completely original way, maintaining the core meaning but using different vocabulary and structure: \\\"{sentence_to_rewrite}\\\"\"\r\n    gemini_api_url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent?key={api_key}\"\r\n\r\n    payload = {\"contents\": [{\"parts\": [{\"text\": prompt}]}]}\r\n\r\n    try:\r\n        response = requests.post(gemini_api_url, json=payload)\r\n        response.raise_for_status()\r\n        result = response.json()\r\n        rewritten_text = (\r\n            result.get('candidates', [{}])[0]\r\n            .get('content', {})\r\n            .get('parts', [{}])[0]\r\n            .get('text', 'Could not generate a suggestion.')\r\n        )\r\n        return jsonify({'rewritten_text': rewritten_text.strip()})\r\n    except Exception as e:\r\n        print(f\"Error calling Gemini API: {e}\")\r\n        return jsonify({'error': 'Failed to get suggestion.'}), 500\r\n\r\n# ---------------------------------------\r\n# 4. RUN SERVER\r\n# ---------------------------------------\r\nif __name__ == '__main__':\r\n    app.run(debug=True, port=5001)\r\n"
        }
    ]
}